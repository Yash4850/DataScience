{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O\n",
    "import warnings # To ignore the warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "train=pd.read_csv(\"CE802_P3_Data.csv\")\n",
    "test=pd.read_csv(\"CE802_P3_Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61.22</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>UK</td>\n",
       "      <td>-14.91</td>\n",
       "      <td>1030.95</td>\n",
       "      <td>614.70</td>\n",
       "      <td>-8.40</td>\n",
       "      <td>11.96</td>\n",
       "      <td>275.99</td>\n",
       "      <td>-333.60</td>\n",
       "      <td>1.86</td>\n",
       "      <td>19.48</td>\n",
       "      <td>6</td>\n",
       "      <td>Very high</td>\n",
       "      <td>7841.50</td>\n",
       "      <td>1605.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.58</td>\n",
       "      <td>12</td>\n",
       "      <td>20.07</td>\n",
       "      <td>Europe</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1059.87</td>\n",
       "      <td>1354.00</td>\n",
       "      <td>-9.97</td>\n",
       "      <td>3.80</td>\n",
       "      <td>347.10</td>\n",
       "      <td>-356.04</td>\n",
       "      <td>6.39</td>\n",
       "      <td>22.15</td>\n",
       "      <td>2</td>\n",
       "      <td>Very low</td>\n",
       "      <td>25589.98</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75.95</td>\n",
       "      <td>6</td>\n",
       "      <td>45.00</td>\n",
       "      <td>USA</td>\n",
       "      <td>-1.29</td>\n",
       "      <td>1320.03</td>\n",
       "      <td>1477.96</td>\n",
       "      <td>-10.02</td>\n",
       "      <td>20.32</td>\n",
       "      <td>345.69</td>\n",
       "      <td>-353.58</td>\n",
       "      <td>13.14</td>\n",
       "      <td>21.87</td>\n",
       "      <td>8</td>\n",
       "      <td>Low</td>\n",
       "      <td>16849.14</td>\n",
       "      <td>3241.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>156.57</td>\n",
       "      <td>12</td>\n",
       "      <td>12.93</td>\n",
       "      <td>USA</td>\n",
       "      <td>-4.83</td>\n",
       "      <td>1696.92</td>\n",
       "      <td>750.14</td>\n",
       "      <td>-10.98</td>\n",
       "      <td>4.78</td>\n",
       "      <td>253.37</td>\n",
       "      <td>-254.37</td>\n",
       "      <td>15.48</td>\n",
       "      <td>28.11</td>\n",
       "      <td>10</td>\n",
       "      <td>Very low</td>\n",
       "      <td>10791.06</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101.27</td>\n",
       "      <td>3</td>\n",
       "      <td>0.51</td>\n",
       "      <td>Rest</td>\n",
       "      <td>-26.28</td>\n",
       "      <td>1451.37</td>\n",
       "      <td>251.06</td>\n",
       "      <td>-9.71</td>\n",
       "      <td>2.06</td>\n",
       "      <td>242.35</td>\n",
       "      <td>-379.77</td>\n",
       "      <td>3.90</td>\n",
       "      <td>23.84</td>\n",
       "      <td>10</td>\n",
       "      <td>Very low</td>\n",
       "      <td>14760.66</td>\n",
       "      <td>336.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       F1  F2     F3      F4     F5       F6       F7     F8     F9     F10  \\\n",
       "0   61.22   6   0.00      UK -14.91  1030.95   614.70  -8.40  11.96  275.99   \n",
       "1   86.58  12  20.07  Europe   0.57  1059.87  1354.00  -9.97   3.80  347.10   \n",
       "2   75.95   6  45.00     USA  -1.29  1320.03  1477.96 -10.02  20.32  345.69   \n",
       "3  156.57  12  12.93     USA  -4.83  1696.92   750.14 -10.98   4.78  253.37   \n",
       "4  101.27   3   0.51    Rest -26.28  1451.37   251.06  -9.71   2.06  242.35   \n",
       "\n",
       "      F11    F12    F13  F14        F15       F16   Target  \n",
       "0 -333.60   1.86  19.48    6  Very high   7841.50  1605.31  \n",
       "1 -356.04   6.39  22.15    2   Very low  25589.98     0.00  \n",
       "2 -353.58  13.14  21.87    8        Low  16849.14  3241.77  \n",
       "3 -254.37  15.48  28.11   10   Very low  10791.06     0.00  \n",
       "4 -379.77   3.90  23.84   10   Very low  14760.66   336.25  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154.97</td>\n",
       "      <td>9</td>\n",
       "      <td>0.57</td>\n",
       "      <td>USA</td>\n",
       "      <td>-14.34</td>\n",
       "      <td>1286.94</td>\n",
       "      <td>1913.38</td>\n",
       "      <td>-10.54</td>\n",
       "      <td>6.66</td>\n",
       "      <td>232.40</td>\n",
       "      <td>-440.10</td>\n",
       "      <td>12.51</td>\n",
       "      <td>22.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Low</td>\n",
       "      <td>22482.82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.99</td>\n",
       "      <td>6</td>\n",
       "      <td>2.31</td>\n",
       "      <td>USA</td>\n",
       "      <td>-16.17</td>\n",
       "      <td>1522.99</td>\n",
       "      <td>1458.10</td>\n",
       "      <td>-12.17</td>\n",
       "      <td>4.96</td>\n",
       "      <td>268.26</td>\n",
       "      <td>-328.74</td>\n",
       "      <td>21.03</td>\n",
       "      <td>20.80</td>\n",
       "      <td>12</td>\n",
       "      <td>High</td>\n",
       "      <td>17183.76</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115.81</td>\n",
       "      <td>6</td>\n",
       "      <td>0.24</td>\n",
       "      <td>UK</td>\n",
       "      <td>6.84</td>\n",
       "      <td>979.23</td>\n",
       "      <td>1427.52</td>\n",
       "      <td>-11.22</td>\n",
       "      <td>4.74</td>\n",
       "      <td>233.43</td>\n",
       "      <td>-404.07</td>\n",
       "      <td>1.17</td>\n",
       "      <td>21.42</td>\n",
       "      <td>6</td>\n",
       "      <td>Very high</td>\n",
       "      <td>17585.36</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.99</td>\n",
       "      <td>9</td>\n",
       "      <td>1023.63</td>\n",
       "      <td>Rest</td>\n",
       "      <td>-12.75</td>\n",
       "      <td>1052.18</td>\n",
       "      <td>605.80</td>\n",
       "      <td>-9.75</td>\n",
       "      <td>11.46</td>\n",
       "      <td>261.27</td>\n",
       "      <td>-506.25</td>\n",
       "      <td>3.99</td>\n",
       "      <td>19.64</td>\n",
       "      <td>4</td>\n",
       "      <td>High</td>\n",
       "      <td>14621.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.39</td>\n",
       "      <td>6</td>\n",
       "      <td>1.68</td>\n",
       "      <td>Europe</td>\n",
       "      <td>-10.98</td>\n",
       "      <td>1235.64</td>\n",
       "      <td>-208.92</td>\n",
       "      <td>-11.45</td>\n",
       "      <td>12.76</td>\n",
       "      <td>332.18</td>\n",
       "      <td>-196.89</td>\n",
       "      <td>25.35</td>\n",
       "      <td>19.50</td>\n",
       "      <td>8</td>\n",
       "      <td>Medium</td>\n",
       "      <td>14624.56</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       F1  F2       F3      F4     F5       F6       F7     F8     F9     F10  \\\n",
       "0  154.97   9     0.57     USA -14.34  1286.94  1913.38 -10.54   6.66  232.40   \n",
       "1   78.99   6     2.31     USA -16.17  1522.99  1458.10 -12.17   4.96  268.26   \n",
       "2  115.81   6     0.24      UK   6.84   979.23  1427.52 -11.22   4.74  233.43   \n",
       "3   48.99   9  1023.63    Rest -12.75  1052.18   605.80  -9.75  11.46  261.27   \n",
       "4   71.39   6     1.68  Europe -10.98  1235.64  -208.92 -11.45  12.76  332.18   \n",
       "\n",
       "      F11    F12    F13  F14        F15       F16  Target  \n",
       "0 -440.10  12.51  22.99    4        Low  22482.82     NaN  \n",
       "1 -328.74  21.03  20.80   12       High  17183.76     NaN  \n",
       "2 -404.07   1.17  21.42    6  Very high  17585.36     NaN  \n",
       "3 -506.25   3.99  19.64    4       High  14621.10     NaN  \n",
       "4 -196.89  25.35  19.50    8     Medium  14624.56     NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing & Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   F1      1500 non-null   float64\n",
      " 1   F2      1500 non-null   int64  \n",
      " 2   F3      1500 non-null   float64\n",
      " 3   F4      1500 non-null   object \n",
      " 4   F5      1500 non-null   float64\n",
      " 5   F6      1500 non-null   float64\n",
      " 6   F7      1500 non-null   float64\n",
      " 7   F8      1500 non-null   float64\n",
      " 8   F9      1500 non-null   float64\n",
      " 9   F10     1500 non-null   float64\n",
      " 10  F11     1500 non-null   float64\n",
      " 11  F12     1500 non-null   float64\n",
      " 12  F13     1500 non-null   float64\n",
      " 13  F14     1500 non-null   int64  \n",
      " 14  F15     1500 non-null   object \n",
      " 15  F16     1500 non-null   float64\n",
      " 16  Target  1500 non-null   float64\n",
      "dtypes: float64(13), int64(2), object(2)\n",
      "memory usage: 199.3+ KB\n",
      "----------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 17 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   F1      1500 non-null   float64\n",
      " 1   F2      1500 non-null   int64  \n",
      " 2   F3      1500 non-null   float64\n",
      " 3   F4      1500 non-null   object \n",
      " 4   F5      1500 non-null   float64\n",
      " 5   F6      1500 non-null   float64\n",
      " 6   F7      1500 non-null   float64\n",
      " 7   F8      1500 non-null   float64\n",
      " 8   F9      1500 non-null   float64\n",
      " 9   F10     1500 non-null   float64\n",
      " 10  F11     1500 non-null   float64\n",
      " 11  F12     1500 non-null   float64\n",
      " 12  F13     1500 non-null   float64\n",
      " 13  F14     1500 non-null   int64  \n",
      " 14  F15     1500 non-null   object \n",
      " 15  F16     1500 non-null   float64\n",
      " 16  Target  0 non-null      float64\n",
      "dtypes: float64(13), int64(2), object(2)\n",
      "memory usage: 199.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info() # no null values in train data\n",
    "print(\"----------------------------------------------\")\n",
    "test.info() #no null values in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F16</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>93.168033</td>\n",
       "      <td>11.828000</td>\n",
       "      <td>190.808320</td>\n",
       "      <td>-6.788620</td>\n",
       "      <td>1376.728553</td>\n",
       "      <td>1092.897040</td>\n",
       "      <td>-10.495153</td>\n",
       "      <td>8.109187</td>\n",
       "      <td>274.393133</td>\n",
       "      <td>-345.797880</td>\n",
       "      <td>12.124980</td>\n",
       "      <td>20.426667</td>\n",
       "      <td>7.894667</td>\n",
       "      <td>16633.036680</td>\n",
       "      <td>922.082533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>29.857614</td>\n",
       "      <td>5.430108</td>\n",
       "      <td>2322.462888</td>\n",
       "      <td>9.072751</td>\n",
       "      <td>301.420683</td>\n",
       "      <td>592.936195</td>\n",
       "      <td>3.040026</td>\n",
       "      <td>5.812853</td>\n",
       "      <td>42.241393</td>\n",
       "      <td>87.920436</td>\n",
       "      <td>8.681726</td>\n",
       "      <td>4.183142</td>\n",
       "      <td>3.673405</td>\n",
       "      <td>8886.071078</td>\n",
       "      <td>1092.331874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-25.550000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-35.070000</td>\n",
       "      <td>350.680000</td>\n",
       "      <td>-1035.560000</td>\n",
       "      <td>-20.200000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>83.890000</td>\n",
       "      <td>-653.160000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>-7.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-22890.400000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>73.120000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>-13.140000</td>\n",
       "      <td>1175.880000</td>\n",
       "      <td>682.885000</td>\n",
       "      <td>-12.475000</td>\n",
       "      <td>3.815000</td>\n",
       "      <td>253.320000</td>\n",
       "      <td>-403.890000</td>\n",
       "      <td>5.835000</td>\n",
       "      <td>18.535000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>12544.200000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>93.215000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.030000</td>\n",
       "      <td>-6.780000</td>\n",
       "      <td>1373.145000</td>\n",
       "      <td>1098.550000</td>\n",
       "      <td>-10.600000</td>\n",
       "      <td>6.740000</td>\n",
       "      <td>274.455000</td>\n",
       "      <td>-343.380000</td>\n",
       "      <td>10.245000</td>\n",
       "      <td>20.445000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>16635.150000</td>\n",
       "      <td>429.570000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>114.035000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>20.415000</td>\n",
       "      <td>-0.420000</td>\n",
       "      <td>1585.802500</td>\n",
       "      <td>1498.155000</td>\n",
       "      <td>-8.540000</td>\n",
       "      <td>10.945000</td>\n",
       "      <td>294.392500</td>\n",
       "      <td>-286.860000</td>\n",
       "      <td>16.477500</td>\n",
       "      <td>22.405000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>21105.420000</td>\n",
       "      <td>1530.205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>198.390000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>82423.590000</td>\n",
       "      <td>20.280000</td>\n",
       "      <td>2427.610000</td>\n",
       "      <td>3081.120000</td>\n",
       "      <td>-0.430000</td>\n",
       "      <td>45.140000</td>\n",
       "      <td>555.360000</td>\n",
       "      <td>-76.740000</td>\n",
       "      <td>70.170000</td>\n",
       "      <td>36.680000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>57340.060000</td>\n",
       "      <td>3960.010000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                F1           F2            F3           F5           F6  \\\n",
       "count  1500.000000  1500.000000   1500.000000  1500.000000  1500.000000   \n",
       "mean     93.168033    11.828000    190.808320    -6.788620  1376.728553   \n",
       "std      29.857614     5.430108   2322.462888     9.072751   301.420683   \n",
       "min     -25.550000     0.000000      0.000000   -35.070000   350.680000   \n",
       "25%      73.120000     9.000000      0.390000   -13.140000  1175.880000   \n",
       "50%      93.215000    12.000000      3.030000    -6.780000  1373.145000   \n",
       "75%     114.035000    15.000000     20.415000    -0.420000  1585.802500   \n",
       "max     198.390000    30.000000  82423.590000    20.280000  2427.610000   \n",
       "\n",
       "                F7           F8           F9          F10          F11  \\\n",
       "count  1500.000000  1500.000000  1500.000000  1500.000000  1500.000000   \n",
       "mean   1092.897040   -10.495153     8.109187   274.393133  -345.797880   \n",
       "std     592.936195     3.040026     5.812853    42.241393    87.920436   \n",
       "min   -1035.560000   -20.200000     0.080000    83.890000  -653.160000   \n",
       "25%     682.885000   -12.475000     3.815000   253.320000  -403.890000   \n",
       "50%    1098.550000   -10.600000     6.740000   274.455000  -343.380000   \n",
       "75%    1498.155000    -8.540000    10.945000   294.392500  -286.860000   \n",
       "max    3081.120000    -0.430000    45.140000   555.360000   -76.740000   \n",
       "\n",
       "               F12          F13          F14           F16       Target  \n",
       "count  1500.000000  1500.000000  1500.000000   1500.000000  1500.000000  \n",
       "mean     12.124980    20.426667     7.894667  16633.036680   922.082533  \n",
       "std       8.681726     4.183142     3.673405   8886.071078  1092.331874  \n",
       "min       0.120000    -7.190000     0.000000 -22890.400000     0.000000  \n",
       "25%       5.835000    18.535000     6.000000  12544.200000     0.000000  \n",
       "50%      10.245000    20.445000     8.000000  16635.150000   429.570000  \n",
       "75%      16.477500    22.405000    10.000000  21105.420000  1530.205000  \n",
       "max      70.170000    36.680000    22.000000  57340.060000  3960.010000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Correlation Coefficients w.r.t Target')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting correlations\n",
    "num_feat=train.columns[train.dtypes!=object]\n",
    "num_feat=num_feat[0:-1] \n",
    "labels = []\n",
    "values = []\n",
    "for col in num_feat:\n",
    "    labels.append(col)\n",
    "    values.append(np.corrcoef(train[col].values, train['Target'].values)[0,1])\n",
    "ind = np.arange(len(labels))\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "rects = ax.barh(ind, np.array(values), color='red')\n",
    "ax.set_yticks(ind+((width)/2))\n",
    "ax.set_yticklabels(labels, rotation='horizontal')\n",
    "ax.set_xlabel(\"Correlation coefficient\")\n",
    "ax.set_title(\"Correlation Coefficients w.r.t Target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['UK' 'Europe' 'USA' 'Rest']\n",
      "['Very high' 'Very low' 'Low' 'Medium' 'High']\n",
      "['USA' 'UK' 'Rest' 'Europe']\n",
      "['Low' 'High' 'Very high' 'Medium' 'Very low']\n"
     ]
    }
   ],
   "source": [
    "# Categorical Values\n",
    "print(train['F4'].unique())\n",
    "print(train['F15'].unique())\n",
    "print(test['F4'].unique())\n",
    "print(test['F15'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USA       407\n",
      "Europe    380\n",
      "Rest      360\n",
      "UK        353\n",
      "Name: F4, dtype: int64\n",
      "Very low     312\n",
      "Low          310\n",
      "High         301\n",
      "Very high    290\n",
      "Medium       287\n",
      "Name: F15, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking the count of each unique value\n",
    "print(train['F4'].value_counts())\n",
    "print(train['F15'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating train output variable\n",
    "X_train = train.drop('Target',axis=1)\n",
    "Y_train = train['Target']\n",
    "\n",
    "# seperating test output variable\n",
    "X_test = test.drop('Target',axis=1)\n",
    "Y_test= test['Target'] \n",
    "\n",
    "# Seperating numerical features from Categorical \n",
    "train_num = X_train.drop(['F4','F15'],axis = 1)\n",
    "test_num = X_test.drop(['F4','F15'],axis = 1)\n",
    "\n",
    "# Normalizing the numerical features in train data\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(train_num.values)\n",
    "X_train = pd.DataFrame(X_train_scaled, columns = ['F1','F2','F3','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F16'])\n",
    "\n",
    "# Normalizing the numerical features in test data\n",
    "X_test_scaled = scaler.fit_transform(test_num.values)\n",
    "X_test = pd.DataFrame(X_test_scaled, columns = ['F1','F2','F3','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F16'])\n",
    "\n",
    "# Converting Categorical to numerical values and joining them\n",
    "train_dummies = pd.get_dummies(train['F4'])\n",
    "train_dummies1 = pd.get_dummies(train['F15'], drop_first=True)\n",
    "X_train = X_train.join([train_dummies,train_dummies1])\n",
    "test_dummies = pd.get_dummies(test['F4'])\n",
    "test_dummies1 = pd.get_dummies(test['F15'], drop_first=True)\n",
    "X_test = X_test.join([test_dummies,test_dummies1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building & Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 389.51396000000005\n",
      "MSE: 249271.9028686667\n",
      "R2: 0.7909482813118549\n"
     ]
    }
   ],
   "source": [
    "Regressor1 = LinearRegression()\n",
    "# Fitting the model\n",
    "Regressor1.fit(X_train,Y_train)\n",
    "# Evaluation Metrics\n",
    "y_pred = Regressor1.predict(X_train)\n",
    "print('MAE:', metrics.mean_absolute_error(Y_train, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(Y_train, y_pred))\n",
    "print('R2:', metrics.r2_score(Y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters =  {'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 'auto', 'max_depth': 15}\n"
     ]
    }
   ],
   "source": [
    "Regressor2 = RandomForestRegressor()\n",
    "# Taking the Hyper parameters\n",
    "parameters = [{'n_estimators' : [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)], # Number of trees in random forest\n",
    "               'max_features' : ['auto', 'sqrt'],# Number of features to consider at every split\n",
    "               'max_depth' : [int(x) for x in np.linspace(5, 30, num = 6)],# Maximum number of levels in tree\n",
    "               'min_samples_split' : [2, 5, 10, 15, 100],# Minimum number of samples required to split a node\n",
    "               'min_samples_leaf' : [1, 2, 5, 10]# Minimum number of samples required at each leaf node\n",
    "              }]\n",
    "# Applying RandomizedSearchCV to find the best model and the best parameters\n",
    "RandomSearch2 = RandomizedSearchCV(estimator = Regressor2, \n",
    "                               param_distributions = parameters,scoring='neg_mean_absolute_error',\n",
    "                               n_iter = 10, cv = 5, verbose=0, random_state=42, n_jobs = -1)\n",
    "# Fitting the model\n",
    "RandomSearch2.fit(X_train,Y_train)\n",
    "print(\"Best Parameters = \", RandomSearch2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 267.1420836669769\n",
      "MSE: 139067.17350361843\n",
      "R2: 0.8833714056840528\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics\n",
    "y_pred = RandomSearch2.predict(X_train)\n",
    "print('MAE:', metrics.mean_absolute_error(Y_train, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(Y_train, y_pred))\n",
    "print('R2:', metrics.r2_score(Y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters =  {'n_neighbors': 3}\n"
     ]
    }
   ],
   "source": [
    "Regressor3 = KNeighborsRegressor()\n",
    "# Taking the Hyper parameters\n",
    "parameters = [{'n_neighbors': range(1,10,1)}]\n",
    "# Applying RandomizedSearchCV to find the best model and the best parameters\n",
    "RandomSearch3 = RandomizedSearchCV(estimator = Regressor3, \n",
    "                               param_distributions = parameters,scoring='neg_mean_absolute_error',\n",
    "                               n_iter = 10, cv = 5, verbose=0, random_state=42, n_jobs = -1)\n",
    "# Fitting the model\n",
    "RandomSearch3.fit(X_train,Y_train)\n",
    "print(\"Best Parameters = \", RandomSearch3.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 363.2856088888889\n",
      "MSE: 284573.23415345186\n",
      "R2: 0.7613428428643773\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics\n",
    "y_pred = RandomSearch3.predict(X_train)\n",
    "print('MAE:', metrics.mean_absolute_error(Y_train, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(Y_train, y_pred))\n",
    "print('R2:', metrics.r2_score(Y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters =  {'kernel': 'rbf', 'gamma': 0.4, 'C': 17000}\n"
     ]
    }
   ],
   "source": [
    "Regressor4 = SVR()\n",
    "# Taking the Hyper parameters\n",
    "parameters = [{'C': [100,1000,10000,14000,15000,16000,17000], 'kernel': ['rbf'],'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
    "# Applying RandomizedSearchCV to find the best model and the best parameters\n",
    "RandomSearch4 = RandomizedSearchCV(estimator = Regressor4, \n",
    "                               param_distributions = parameters,scoring='neg_mean_absolute_error',\n",
    "                               n_iter = 10, cv = 5, verbose=0, random_state=42, n_jobs = -1)\n",
    "# Fitting the model\n",
    "RandomSearch4.fit(X_train,Y_train)\n",
    "print(\"Best Parameters = \", RandomSearch4.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 104.29676251901901\n",
      "MSE: 54600.864134688614\n",
      "R2: 0.9542090209211086\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Metrics\n",
    "y_pred = RandomSearch4.predict(X_train)\n",
    "print('MAE:', metrics.mean_absolute_error(Y_train, y_pred))\n",
    "print('MSE:', metrics.mean_squared_error(Y_train, y_pred))\n",
    "print('R2:', metrics.r2_score(Y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We took Relu function as it clips the negative values\n",
    "def create_model():\n",
    "    '''\n",
    "    Create a neural network\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=X_train.shape[1], activation=\"relu\", kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.2)) # We are dropping a few neurons for generalizing the model\n",
    "    model.add(Dense(256, input_dim=X_train.shape[1], activation=\"relu\", kernel_initializer='normal'))\n",
    "    model.add(Dense(256, input_dim=X_train.shape[1], activation=\"relu\", kernel_initializer='normal'))\n",
    "    model.add(Dense(1, activation=\"relu\", kernel_initializer='normal'))\n",
    "    adam = Adam(lr=1e-3, decay=1e-3)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[rmse])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def rmse(y_true, y_pred):\n",
    "    '''\n",
    "    RMSE calculus to use during training phase\n",
    "    '''\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_45 (Dense)             (None, 256)               5888      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 137,729\n",
      "Trainable params: 137,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters and create the model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "batch_size = 256\n",
    "nb_epoch = 1000\n",
    "\n",
    "print('Build model...')\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model...\n",
      "Epoch 1/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2141613.5000 - rmse: 1463.4253\n",
      "Epoch 00001: val_loss improved from inf to 1914138.50000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 31ms/step - loss: 2074349.8750 - rmse: 1435.8168 - val_loss: 1914138.5000 - val_rmse: 1541.0476\n",
      "Epoch 2/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2201856.5000 - rmse: 1483.8654\n",
      "Epoch 00002: val_loss improved from 1914138.50000 to 1912081.50000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2073196.7500 - rmse: 1437.8926 - val_loss: 1912081.5000 - val_rmse: 1540.2661\n",
      "Epoch 3/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2246252.0000 - rmse: 1498.7501\n",
      "Epoch 00003: val_loss improved from 1912081.50000 to 1906413.50000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2069809.5000 - rmse: 1440.8228 - val_loss: 1906413.5000 - val_rmse: 1538.1121\n",
      "Epoch 4/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2303942.7500 - rmse: 1517.8744\n",
      "Epoch 00004: val_loss improved from 1906413.50000 to 1893078.37500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2061175.0000 - rmse: 1430.1128 - val_loss: 1893078.3750 - val_rmse: 1533.0291\n",
      "Epoch 5/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1714841.3750 - rmse: 1309.5195\n",
      "Epoch 00005: val_loss improved from 1893078.37500 to 1865170.75000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2041215.1250 - rmse: 1432.7142 - val_loss: 1865170.7500 - val_rmse: 1522.3196\n",
      "Epoch 6/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2025326.7500 - rmse: 1423.1398\n",
      "Epoch 00006: val_loss improved from 1865170.75000 to 1812362.62500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2002421.5000 - rmse: 1407.6775 - val_loss: 1812362.6250 - val_rmse: 1501.7535\n",
      "Epoch 7/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2101032.0000 - rmse: 1449.4937\n",
      "Epoch 00007: val_loss improved from 1812362.62500 to 1721871.75000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1931334.3750 - rmse: 1386.0925 - val_loss: 1721871.7500 - val_rmse: 1465.5300\n",
      "Epoch 8/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1784630.5000 - rmse: 1335.9006\n",
      "Epoch 00008: val_loss improved from 1721871.75000 to 1584118.00000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1812463.1250 - rmse: 1336.0256 - val_loss: 1584118.0000 - val_rmse: 1407.5983\n",
      "Epoch 9/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1920415.6250 - rmse: 1385.7906\n",
      "Epoch 00009: val_loss improved from 1584118.00000 to 1406160.37500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1642144.3750 - rmse: 1280.5045 - val_loss: 1406160.3750 - val_rmse: 1325.9133\n",
      "Epoch 10/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1744831.0000 - rmse: 1320.9205\n",
      "Epoch 00010: val_loss improved from 1406160.37500 to 1236779.37500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1437151.0000 - rmse: 1192.0679 - val_loss: 1236779.3750 - val_rmse: 1233.7925\n",
      "Epoch 11/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1141069.2500 - rmse: 1068.2084\n",
      "Epoch 00011: val_loss improved from 1236779.37500 to 1180128.37500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1259078.2500 - rmse: 1121.4685 - val_loss: 1180128.3750 - val_rmse: 1175.2905\n",
      "Epoch 12/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1355296.1250 - rmse: 1164.1719\n",
      "Epoch 00012: val_loss did not improve from 1180128.37500\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1234446.1250 - rmse: 1110.1633 - val_loss: 1238951.8750 - val_rmse: 1172.4675\n",
      "Epoch 13/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1237715.2500 - rmse: 1112.5265\n",
      "Epoch 00013: val_loss did not improve from 1180128.37500\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1247478.8750 - rmse: 1119.7440 - val_loss: 1201748.6250 - val_rmse: 1160.7852\n",
      "Epoch 14/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1225753.8750 - rmse: 1107.1377\n",
      "Epoch 00014: val_loss improved from 1180128.37500 to 1143950.87500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1203010.0000 - rmse: 1097.7860 - val_loss: 1143950.8750 - val_rmse: 1152.1589\n",
      "Epoch 15/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1069959.5000 - rmse: 1034.3884\n",
      "Epoch 00015: val_loss improved from 1143950.87500 to 1124761.87500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1182221.2500 - rmse: 1087.8049 - val_loss: 1124761.8750 - val_rmse: 1154.5068\n",
      "Epoch 16/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1201067.5000 - rmse: 1095.9323\n",
      "Epoch 00016: val_loss improved from 1124761.87500 to 1111930.75000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1171933.1250 - rmse: 1073.6976 - val_loss: 1111930.7500 - val_rmse: 1147.6196\n",
      "Epoch 17/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1202038.8750 - rmse: 1096.3754\n",
      "Epoch 00017: val_loss improved from 1111930.75000 to 1098332.25000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1158569.7500 - rmse: 1064.7628 - val_loss: 1098332.2500 - val_rmse: 1135.5757\n",
      "Epoch 18/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1132569.5000 - rmse: 1064.2225\n",
      "Epoch 00018: val_loss improved from 1098332.25000 to 1087898.50000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1140391.2500 - rmse: 1070.4386 - val_loss: 1087898.5000 - val_rmse: 1124.1073\n",
      "Epoch 19/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1133047.7500 - rmse: 1064.4471\n",
      "Epoch 00019: val_loss improved from 1087898.50000 to 1078192.50000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1127051.2500 - rmse: 1058.7670 - val_loss: 1078192.5000 - val_rmse: 1114.2781\n",
      "Epoch 20/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1134927.6250 - rmse: 1065.3298\n",
      "Epoch 00020: val_loss improved from 1078192.50000 to 1064198.50000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 1115346.6250 - rmse: 1057.5989 - val_loss: 1064198.5000 - val_rmse: 1106.0159\n",
      "Epoch 21/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1010228.6250 - rmse: 1005.1013\n",
      "Epoch 00021: val_loss improved from 1064198.50000 to 1046544.62500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 1101305.3750 - rmse: 1052.9425 - val_loss: 1046544.6250 - val_rmse: 1099.0183\n",
      "Epoch 22/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1065866.7500 - rmse: 1032.4082\n",
      "Epoch 00022: val_loss improved from 1046544.62500 to 1029672.75000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1086889.7500 - rmse: 1044.3937 - val_loss: 1029672.7500 - val_rmse: 1091.7339\n",
      "Epoch 23/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 874788.4375 - rmse: 935.3013\n",
      "Epoch 00023: val_loss improved from 1029672.75000 to 1013270.62500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1073081.5000 - rmse: 1038.8989 - val_loss: 1013270.6250 - val_rmse: 1083.4448\n",
      "Epoch 24/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 907370.6875 - rmse: 952.5601\n",
      "Epoch 00024: val_loss improved from 1013270.62500 to 997216.18750, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1057662.1250 - rmse: 1033.7052 - val_loss: 997216.1875 - val_rmse: 1073.2902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1007776.5000 - rmse: 1003.8807\n",
      "Epoch 00025: val_loss improved from 997216.18750 to 981560.31250, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1042850.4375 - rmse: 1025.6350 - val_loss: 981560.3125 - val_rmse: 1060.0250\n",
      "Epoch 26/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1053316.0000 - rmse: 1026.3119\n",
      "Epoch 00026: val_loss improved from 981560.31250 to 963524.06250, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1026267.9375 - rmse: 1008.1172 - val_loss: 963524.0625 - val_rmse: 1048.6287\n",
      "Epoch 27/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1131030.2500 - rmse: 1063.4990\n",
      "Epoch 00027: val_loss improved from 963524.06250 to 942849.37500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1010217.6875 - rmse: 1005.4789 - val_loss: 942849.3750 - val_rmse: 1037.4950\n",
      "Epoch 28/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1130771.0000 - rmse: 1063.3772\n",
      "Epoch 00028: val_loss improved from 942849.37500 to 921880.43750, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 991295.8750 - rmse: 987.5254 - val_loss: 921880.4375 - val_rmse: 1026.4249\n",
      "Epoch 29/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 950632.8750 - rmse: 975.0040\n",
      "Epoch 00029: val_loss improved from 921880.43750 to 900629.75000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 972357.3125 - rmse: 983.6708 - val_loss: 900629.7500 - val_rmse: 1016.5345\n",
      "Epoch 30/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 996230.6250 - rmse: 998.1135\n",
      "Epoch 00030: val_loss improved from 900629.75000 to 879534.31250, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 954419.6250 - rmse: 978.2496 - val_loss: 879534.3125 - val_rmse: 1004.4183\n",
      "Epoch 31/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 944655.3125 - rmse: 971.9338\n",
      "Epoch 00031: val_loss improved from 879534.31250 to 857999.87500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 935355.8125 - rmse: 969.5541 - val_loss: 857999.8750 - val_rmse: 987.7116\n",
      "Epoch 32/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1018528.3125 - rmse: 1009.2216\n",
      "Epoch 00032: val_loss improved from 857999.87500 to 835848.00000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 915187.5000 - rmse: 956.3581 - val_loss: 835848.0000 - val_rmse: 971.3041\n",
      "Epoch 33/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 799402.0625 - rmse: 894.0929\n",
      "Epoch 00033: val_loss improved from 835848.00000 to 813301.87500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 894256.7500 - rmse: 942.9520 - val_loss: 813301.8750 - val_rmse: 955.8788\n",
      "Epoch 34/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 898379.3750 - rmse: 947.8288\n",
      "Epoch 00034: val_loss improved from 813301.87500 to 789322.18750, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 873606.3125 - rmse: 932.4604 - val_loss: 789322.1875 - val_rmse: 941.4647\n",
      "Epoch 35/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 803695.8125 - rmse: 896.4908\n",
      "Epoch 00035: val_loss improved from 789322.18750 to 766199.06250, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 850837.8125 - rmse: 919.4206 - val_loss: 766199.0625 - val_rmse: 928.7719\n",
      "Epoch 36/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 751756.7500 - rmse: 867.0391\n",
      "Epoch 00036: val_loss improved from 766199.06250 to 742679.81250, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 829415.3125 - rmse: 907.4442 - val_loss: 742679.8125 - val_rmse: 913.2435\n",
      "Epoch 37/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 868687.0000 - rmse: 932.0338\n",
      "Epoch 00037: val_loss improved from 742679.81250 to 718560.37500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 806782.3125 - rmse: 898.7159 - val_loss: 718560.3750 - val_rmse: 893.2552\n",
      "Epoch 38/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 865466.6250 - rmse: 930.3046\n",
      "Epoch 00038: val_loss improved from 718560.37500 to 693431.75000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 782472.2500 - rmse: 882.7415 - val_loss: 693431.7500 - val_rmse: 875.5771\n",
      "Epoch 39/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 644677.5000 - rmse: 802.9181\n",
      "Epoch 00039: val_loss improved from 693431.75000 to 667968.50000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 756282.9375 - rmse: 871.5497 - val_loss: 667968.5000 - val_rmse: 860.8918\n",
      "Epoch 40/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 714524.7500 - rmse: 845.2957\n",
      "Epoch 00040: val_loss improved from 667968.50000 to 640246.00000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 730619.8125 - rmse: 857.0863 - val_loss: 640246.0000 - val_rmse: 839.1602\n",
      "Epoch 41/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 706894.0625 - rmse: 840.7699\n",
      "Epoch 00041: val_loss improved from 640246.00000 to 611996.56250, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 701041.4375 - rmse: 840.3048 - val_loss: 611996.5625 - val_rmse: 820.4854\n",
      "Epoch 42/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 714317.2500 - rmse: 845.1729\n",
      "Epoch 00042: val_loss improved from 611996.56250 to 581252.50000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 671948.0625 - rmse: 816.2010 - val_loss: 581252.5000 - val_rmse: 795.7220\n",
      "Epoch 43/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 647825.3750 - rmse: 804.8760\n",
      "Epoch 00043: val_loss improved from 581252.50000 to 549395.75000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 637815.4375 - rmse: 798.5844 - val_loss: 549395.7500 - val_rmse: 775.8771\n",
      "Epoch 44/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 631827.6250 - rmse: 794.8759\n",
      "Epoch 00044: val_loss improved from 549395.75000 to 514927.18750, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 601953.6250 - rmse: 777.1613 - val_loss: 514927.1875 - val_rmse: 747.5662\n",
      "Epoch 45/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 615573.5000 - rmse: 784.5849\n",
      "Epoch 00045: val_loss improved from 514927.18750 to 478068.00000, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 565496.5625 - rmse: 751.2979 - val_loss: 478068.0000 - val_rmse: 719.5231\n",
      "Epoch 46/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 482553.4375 - rmse: 694.6606\n",
      "Epoch 00046: val_loss improved from 478068.00000 to 440078.40625, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 525442.4375 - rmse: 722.9620 - val_loss: 440078.4062 - val_rmse: 689.8068\n",
      "Epoch 47/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 478571.8125 - rmse: 691.7889\n",
      "Epoch 00047: val_loss improved from 440078.40625 to 402682.28125, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 483572.3125 - rmse: 695.3763 - val_loss: 402682.2812 - val_rmse: 662.8965\n",
      "Epoch 48/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 463279.1875 - rmse: 680.6462\n",
      "Epoch 00048: val_loss improved from 402682.28125 to 363004.87500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 441675.2188 - rmse: 664.0428 - val_loss: 363004.8750 - val_rmse: 626.1564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 405712.8438 - rmse: 636.9559\n",
      "Epoch 00049: val_loss improved from 363004.87500 to 324616.37500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 399765.2500 - rmse: 630.4017 - val_loss: 324616.3750 - val_rmse: 591.4174\n",
      "Epoch 50/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 384329.2500 - rmse: 619.9429\n",
      "Epoch 00050: val_loss improved from 324616.37500 to 289180.65625, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 355874.2812 - rmse: 595.9944 - val_loss: 289180.6562 - val_rmse: 564.6346\n",
      "Epoch 51/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 339616.1250 - rmse: 582.7659\n",
      "Epoch 00051: val_loss improved from 289180.65625 to 253997.57812, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 318503.1875 - rmse: 561.6188 - val_loss: 253997.5781 - val_rmse: 527.0385\n",
      "Epoch 52/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 278279.2500 - rmse: 527.5218\n",
      "Epoch 00052: val_loss improved from 253997.57812 to 223760.23438, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 280555.1562 - rmse: 529.2059 - val_loss: 223760.2344 - val_rmse: 497.3868\n",
      "Epoch 53/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 230762.2500 - rmse: 480.3772\n",
      "Epoch 00053: val_loss improved from 223760.23438 to 198197.45312, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 247277.8125 - rmse: 496.1588 - val_loss: 198197.4531 - val_rmse: 469.7855\n",
      "Epoch 54/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 233085.4062 - rmse: 482.7892\n",
      "Epoch 00054: val_loss improved from 198197.45312 to 175614.26562, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 220939.8594 - rmse: 470.1957 - val_loss: 175614.2656 - val_rmse: 444.9739\n",
      "Epoch 55/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 166351.9531 - rmse: 407.8627\n",
      "Epoch 00055: val_loss improved from 175614.26562 to 157792.67188, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 197799.0625 - rmse: 442.6800 - val_loss: 157792.6719 - val_rmse: 421.0757\n",
      "Epoch 56/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 174768.9844 - rmse: 418.0538\n",
      "Epoch 00056: val_loss improved from 157792.67188 to 143818.35938, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 178982.7188 - rmse: 424.2298 - val_loss: 143818.3594 - val_rmse: 406.1045\n",
      "Epoch 57/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 161027.4062 - rmse: 401.2822\n",
      "Epoch 00057: val_loss improved from 143818.35938 to 133549.03125, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 163564.4375 - rmse: 403.8319 - val_loss: 133549.0312 - val_rmse: 391.2419\n",
      "Epoch 58/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 140187.5000 - rmse: 374.4162\n",
      "Epoch 00058: val_loss improved from 133549.03125 to 124093.89062, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 153784.3750 - rmse: 391.0291 - val_loss: 124093.8906 - val_rmse: 381.7819\n",
      "Epoch 59/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 158969.9375 - rmse: 398.7104\n",
      "Epoch 00059: val_loss improved from 124093.89062 to 118331.54688, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 144244.4688 - rmse: 378.1955 - val_loss: 118331.5469 - val_rmse: 377.3646\n",
      "Epoch 60/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 139784.9375 - rmse: 373.8782\n",
      "Epoch 00060: val_loss improved from 118331.54688 to 113536.64062, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 137124.0938 - rmse: 370.1215 - val_loss: 113536.6406 - val_rmse: 371.8633\n",
      "Epoch 61/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 141750.3125 - rmse: 376.4974\n",
      "Epoch 00061: val_loss improved from 113536.64062 to 108123.77344, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 130677.2422 - rmse: 358.5285 - val_loss: 108123.7734 - val_rmse: 364.8214\n",
      "Epoch 62/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 118080.8594 - rmse: 343.6290\n",
      "Epoch 00062: val_loss improved from 108123.77344 to 103839.69531, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 125614.7188 - rmse: 353.7805 - val_loss: 103839.6953 - val_rmse: 358.4453\n",
      "Epoch 63/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 115968.6094 - rmse: 340.5416\n",
      "Epoch 00063: val_loss improved from 103839.69531 to 99697.97656, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 120353.4141 - rmse: 348.6116 - val_loss: 99697.9766 - val_rmse: 352.5325\n",
      "Epoch 64/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 134358.1562 - rmse: 366.5490\n",
      "Epoch 00064: val_loss improved from 99697.97656 to 96006.15625, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 116846.8281 - rmse: 339.0832 - val_loss: 96006.1562 - val_rmse: 347.2014\n",
      "Epoch 65/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 121491.7344 - rmse: 348.5566\n",
      "Epoch 00065: val_loss improved from 96006.15625 to 93184.61719, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 113159.7734 - rmse: 336.5081 - val_loss: 93184.6172 - val_rmse: 343.7297\n",
      "Epoch 66/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 103152.7812 - rmse: 321.1741\n",
      "Epoch 00066: val_loss improved from 93184.61719 to 90557.60156, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 108698.2422 - rmse: 331.4014 - val_loss: 90557.6016 - val_rmse: 340.1612\n",
      "Epoch 67/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 97606.5156 - rmse: 312.4204\n",
      "Epoch 00067: val_loss improved from 90557.60156 to 88234.87500, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 104352.7969 - rmse: 321.4874 - val_loss: 88234.8750 - val_rmse: 336.6505\n",
      "Epoch 68/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 95965.8125 - rmse: 309.7835\n",
      "Epoch 00068: val_loss improved from 88234.87500 to 85664.38281, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 102713.9219 - rmse: 320.5298 - val_loss: 85664.3828 - val_rmse: 332.9235\n",
      "Epoch 69/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 110071.7344 - rmse: 331.7706\n",
      "Epoch 00069: val_loss improved from 85664.38281 to 82449.07031, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 100259.7031 - rmse: 314.7343 - val_loss: 82449.0703 - val_rmse: 326.3373\n",
      "Epoch 70/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 96289.1875 - rmse: 310.3050\n",
      "Epoch 00070: val_loss improved from 82449.07031 to 81344.57812, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 96167.7969 - rmse: 307.3564 - val_loss: 81344.5781 - val_rmse: 324.9920\n",
      "Epoch 71/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 98499.0859 - rmse: 313.8456\n",
      "Epoch 00071: val_loss improved from 81344.57812 to 78416.33594, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 94154.5859 - rmse: 307.1898 - val_loss: 78416.3359 - val_rmse: 320.4228\n",
      "Epoch 72/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 91174.5000 - rmse: 301.9511\n",
      "Epoch 00072: val_loss improved from 78416.33594 to 77497.65625, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 92011.5625 - rmse: 302.7276 - val_loss: 77497.6562 - val_rmse: 320.9685\n",
      "Epoch 73/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 103686.4141 - rmse: 322.0038\n",
      "Epoch 00073: val_loss improved from 77497.65625 to 75717.07812, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 88528.3828 - rmse: 297.4958 - val_loss: 75717.0781 - val_rmse: 318.8805\n",
      "Epoch 74/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 91937.6172 - rmse: 303.2122\n",
      "Epoch 00074: val_loss improved from 75717.07812 to 74222.75781, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 87044.1719 - rmse: 293.9254 - val_loss: 74222.7578 - val_rmse: 316.7168\n",
      "Epoch 75/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 106127.2656 - rmse: 325.7718\n",
      "Epoch 00075: val_loss improved from 74222.75781 to 72212.26562, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 83964.1484 - rmse: 288.5271 - val_loss: 72212.2656 - val_rmse: 312.9412\n",
      "Epoch 76/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 78006.3203 - rmse: 279.2961\n",
      "Epoch 00076: val_loss improved from 72212.26562 to 70745.52344, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 82182.6094 - rmse: 288.5403 - val_loss: 70745.5234 - val_rmse: 310.0790\n",
      "Epoch 77/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 81420.0469 - rmse: 285.3420\n",
      "Epoch 00077: val_loss improved from 70745.52344 to 68677.36719, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 81090.6016 - rmse: 281.7565 - val_loss: 68677.3672 - val_rmse: 305.4090\n",
      "Epoch 78/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 77862.0078 - rmse: 279.0377\n",
      "Epoch 00078: val_loss improved from 68677.36719 to 67389.20312, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 78761.9297 - rmse: 276.1773 - val_loss: 67389.2031 - val_rmse: 303.3692\n",
      "Epoch 79/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 61704.7500 - rmse: 248.4044\n",
      "Epoch 00079: val_loss improved from 67389.20312 to 65719.79688, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 76489.5859 - rmse: 272.7879 - val_loss: 65719.7969 - val_rmse: 300.9447\n",
      "Epoch 80/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 71776.2109 - rmse: 267.9108\n",
      "Epoch 00080: val_loss improved from 65719.79688 to 64096.82812, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 75040.6875 - rmse: 273.6207 - val_loss: 64096.8281 - val_rmse: 297.7867\n",
      "Epoch 81/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 62987.4609 - rmse: 250.9730\n",
      "Epoch 00081: val_loss improved from 64096.82812 to 62935.44141, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 73333.0234 - rmse: 267.1472 - val_loss: 62935.4414 - val_rmse: 295.7984\n",
      "Epoch 82/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 56236.6562 - rmse: 237.1427\n",
      "Epoch 00082: val_loss improved from 62935.44141 to 61563.52734, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 72011.5625 - rmse: 265.5267 - val_loss: 61563.5273 - val_rmse: 293.0591\n",
      "Epoch 83/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 60364.5977 - rmse: 245.6921\n",
      "Epoch 00083: val_loss improved from 61563.52734 to 60332.41406, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 70028.6016 - rmse: 266.8218 - val_loss: 60332.4141 - val_rmse: 290.7896\n",
      "Epoch 84/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 77062.4375 - rmse: 277.6012\n",
      "Epoch 00084: val_loss improved from 60332.41406 to 58927.96094, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 68561.3828 - rmse: 261.7013 - val_loss: 58927.9609 - val_rmse: 287.3055\n",
      "Epoch 85/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 65588.4688 - rmse: 256.1024\n",
      "Epoch 00085: val_loss improved from 58927.96094 to 57762.57422, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 67960.6641 - rmse: 258.4911 - val_loss: 57762.5742 - val_rmse: 285.1994\n",
      "Epoch 86/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 61911.8203 - rmse: 248.8209\n",
      "Epoch 00086: val_loss improved from 57762.57422 to 56523.39453, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 66521.8125 - rmse: 258.0906 - val_loss: 56523.3945 - val_rmse: 282.8000\n",
      "Epoch 87/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 51984.3281 - rmse: 228.0007\n",
      "Epoch 00087: val_loss improved from 56523.39453 to 55713.63281, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 65897.7266 - rmse: 251.7493 - val_loss: 55713.6328 - val_rmse: 281.3227\n",
      "Epoch 88/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 54195.8281 - rmse: 232.8000\n",
      "Epoch 00088: val_loss improved from 55713.63281 to 54518.30859, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 65283.3203 - rmse: 256.8665 - val_loss: 54518.3086 - val_rmse: 279.1609\n",
      "Epoch 89/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 75118.9688 - rmse: 274.0784\n",
      "Epoch 00089: val_loss improved from 54518.30859 to 54268.36719, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 63602.8008 - rmse: 252.3472 - val_loss: 54268.3672 - val_rmse: 278.2298\n",
      "Epoch 90/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 62584.0742 - rmse: 250.1681\n",
      "Epoch 00090: val_loss improved from 54268.36719 to 52630.86328, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 62681.1719 - rmse: 248.5111 - val_loss: 52630.8633 - val_rmse: 274.7041\n",
      "Epoch 91/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 46985.6680 - rmse: 216.7618\n",
      "Epoch 00091: val_loss improved from 52630.86328 to 52284.48047, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 60369.6992 - rmse: 247.4141 - val_loss: 52284.4805 - val_rmse: 272.5803\n",
      "Epoch 92/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 43891.8125 - rmse: 209.5037\n",
      "Epoch 00092: val_loss improved from 52284.48047 to 50966.64062, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 57646.2539 - rmse: 239.9141 - val_loss: 50966.6406 - val_rmse: 269.9696\n",
      "Epoch 93/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 97323.7500 - rmse: 311.9676\n",
      "Epoch 00093: val_loss improved from 50966.64062 to 50425.16797, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 57534.0000 - rmse: 234.3674 - val_loss: 50425.1680 - val_rmse: 268.2421\n",
      "Epoch 94/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 37881.6953 - rmse: 194.6322\n",
      "Epoch 00094: val_loss improved from 50425.16797 to 48375.71094, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 55051.6484 - rmse: 236.0641 - val_loss: 48375.7109 - val_rmse: 263.5354\n",
      "Epoch 95/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 41631.3828 - rmse: 204.0377\n",
      "Epoch 00095: val_loss improved from 48375.71094 to 48156.35938, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 54260.0547 - rmse: 231.7565 - val_loss: 48156.3594 - val_rmse: 262.6776\n",
      "Epoch 96/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 37728.5469 - rmse: 194.2384\n",
      "Epoch 00096: val_loss improved from 48156.35938 to 46143.44531, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 53958.6484 - rmse: 230.7431 - val_loss: 46143.4453 - val_rmse: 257.7079\n",
      "Epoch 97/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 51009.1328 - rmse: 225.8520\n",
      "Epoch 00097: val_loss improved from 46143.44531 to 45362.40234, saving model to weights_rossmann.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 9ms/step - loss: 51890.4805 - rmse: 223.2969 - val_loss: 45362.4023 - val_rmse: 255.7484\n",
      "Epoch 98/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 51428.3672 - rmse: 226.7782\n",
      "Epoch 00098: val_loss improved from 45362.40234 to 44393.98828, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 50603.3477 - rmse: 219.0359 - val_loss: 44393.9883 - val_rmse: 253.6355\n",
      "Epoch 99/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 43533.0234 - rmse: 208.6457\n",
      "Epoch 00099: val_loss improved from 44393.98828 to 43561.51172, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 49691.7148 - rmse: 220.2258 - val_loss: 43561.5117 - val_rmse: 251.7724\n",
      "Epoch 100/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 52720.5156 - rmse: 229.6095\n",
      "Epoch 00100: val_loss improved from 43561.51172 to 42420.76953, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 48582.5664 - rmse: 219.9759 - val_loss: 42420.7695 - val_rmse: 248.5291\n",
      "Epoch 101/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 31957.6641 - rmse: 178.7671\n",
      "Epoch 00101: val_loss improved from 42420.76953 to 41646.86328, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 47363.8672 - rmse: 215.3369 - val_loss: 41646.8633 - val_rmse: 246.4007\n",
      "Epoch 102/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 60745.1133 - rmse: 246.4652\n",
      "Epoch 00102: val_loss improved from 41646.86328 to 40665.44141, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 46629.7930 - rmse: 213.0574 - val_loss: 40665.4414 - val_rmse: 243.4910\n",
      "Epoch 103/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 38470.3125 - rmse: 196.1385\n",
      "Epoch 00103: val_loss improved from 40665.44141 to 39970.43750, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 45728.3281 - rmse: 212.0945 - val_loss: 39970.4375 - val_rmse: 241.7139\n",
      "Epoch 104/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 49886.1680 - rmse: 223.3521\n",
      "Epoch 00104: val_loss improved from 39970.43750 to 39068.84766, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 44851.0625 - rmse: 210.6992 - val_loss: 39068.8477 - val_rmse: 239.0147\n",
      "Epoch 105/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 52077.2148 - rmse: 228.2043\n",
      "Epoch 00105: val_loss improved from 39068.84766 to 38285.79688, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 43593.3086 - rmse: 207.6991 - val_loss: 38285.7969 - val_rmse: 236.7990\n",
      "Epoch 106/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 30150.7578 - rmse: 173.6397\n",
      "Epoch 00106: val_loss improved from 38285.79688 to 37274.91406, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 42722.8516 - rmse: 206.5956 - val_loss: 37274.9141 - val_rmse: 233.7319\n",
      "Epoch 107/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 46293.9766 - rmse: 215.1604\n",
      "Epoch 00107: val_loss improved from 37274.91406 to 36294.62109, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 41893.1289 - rmse: 206.0677 - val_loss: 36294.6211 - val_rmse: 230.6605\n",
      "Epoch 108/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 63904.0391 - rmse: 252.7925\n",
      "Epoch 00108: val_loss improved from 36294.62109 to 35636.42969, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 41307.8750 - rmse: 201.1599 - val_loss: 35636.4297 - val_rmse: 228.7531\n",
      "Epoch 109/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 32379.1250 - rmse: 179.9420\n",
      "Epoch 00109: val_loss improved from 35636.42969 to 34925.53516, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 40156.1953 - rmse: 195.4533 - val_loss: 34925.5352 - val_rmse: 226.8602\n",
      "Epoch 110/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 58926.5820 - rmse: 242.7480\n",
      "Epoch 00110: val_loss improved from 34925.53516 to 34206.30859, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 39288.7930 - rmse: 195.3410 - val_loss: 34206.3086 - val_rmse: 224.6521\n",
      "Epoch 111/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 39342.3828 - rmse: 198.3491\n",
      "Epoch 00111: val_loss improved from 34206.30859 to 33558.32031, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 38351.5039 - rmse: 194.6446 - val_loss: 33558.3203 - val_rmse: 222.6446\n",
      "Epoch 112/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 29186.5254 - rmse: 170.8406\n",
      "Epoch 00112: val_loss improved from 33558.32031 to 32828.46094, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 37630.3281 - rmse: 190.8957 - val_loss: 32828.4609 - val_rmse: 220.0231\n",
      "Epoch 113/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 19904.9531 - rmse: 141.0849\n",
      "Epoch 00113: val_loss improved from 32828.46094 to 32109.25391, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 36694.6250 - rmse: 188.7430 - val_loss: 32109.2539 - val_rmse: 218.0960\n",
      "Epoch 114/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 33748.6484 - rmse: 183.7081\n",
      "Epoch 00114: val_loss improved from 32109.25391 to 31340.84961, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 35944.0547 - rmse: 188.5782 - val_loss: 31340.8496 - val_rmse: 215.2299\n",
      "Epoch 115/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 28238.4180 - rmse: 168.0429\n",
      "Epoch 00115: val_loss improved from 31340.84961 to 30649.57422, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 35212.5859 - rmse: 185.3982 - val_loss: 30649.5742 - val_rmse: 212.9578\n",
      "Epoch 116/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 25694.2168 - rmse: 160.2942\n",
      "Epoch 00116: val_loss improved from 30649.57422 to 29985.61914, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 34408.0000 - rmse: 184.2227 - val_loss: 29985.6191 - val_rmse: 211.1004\n",
      "Epoch 117/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 27434.0938 - rmse: 165.6324\n",
      "Epoch 00117: val_loss improved from 29985.61914 to 29349.75586, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 33861.6445 - rmse: 182.7211 - val_loss: 29349.7559 - val_rmse: 209.2797\n",
      "Epoch 118/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 40643.0000 - rmse: 201.6011\n",
      "Epoch 00118: val_loss improved from 29349.75586 to 28794.35938, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 32931.5742 - rmse: 179.6087 - val_loss: 28794.3594 - val_rmse: 207.2411\n",
      "Epoch 119/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 39999.2266 - rmse: 199.9981\n",
      "Epoch 00119: val_loss improved from 28794.35938 to 27906.52930, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 32298.1328 - rmse: 179.4103 - val_loss: 27906.5293 - val_rmse: 204.0517\n",
      "Epoch 120/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 41299.1094 - rmse: 203.2218\n",
      "Epoch 00120: val_loss improved from 27906.52930 to 27374.03906, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 31572.4395 - rmse: 177.4320 - val_loss: 27374.0391 - val_rmse: 202.2853\n",
      "Epoch 121/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 28089.6992 - rmse: 167.5998\n",
      "Epoch 00121: val_loss improved from 27374.03906 to 26805.24805, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 30868.9004 - rmse: 174.6281 - val_loss: 26805.2480 - val_rmse: 199.9235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 27696.5430 - rmse: 166.4228\n",
      "Epoch 00122: val_loss improved from 26805.24805 to 26217.80664, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 30330.9805 - rmse: 175.1659 - val_loss: 26217.8066 - val_rmse: 197.7877\n",
      "Epoch 123/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 20206.6328 - rmse: 142.1500\n",
      "Epoch 00123: val_loss improved from 26217.80664 to 25644.06836, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 29593.1504 - rmse: 171.3332 - val_loss: 25644.0684 - val_rmse: 195.5903\n",
      "Epoch 124/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 36479.0547 - rmse: 190.9949\n",
      "Epoch 00124: val_loss improved from 25644.06836 to 25427.95703, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 29050.7031 - rmse: 165.3145 - val_loss: 25427.9570 - val_rmse: 194.5183\n",
      "Epoch 125/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 21268.6133 - rmse: 145.8376\n",
      "Epoch 00125: val_loss improved from 25427.95703 to 24664.29102, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 28547.2305 - rmse: 164.2373 - val_loss: 24664.2910 - val_rmse: 192.3159\n",
      "Epoch 126/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 39340.3203 - rmse: 198.3439\n",
      "Epoch 00126: val_loss improved from 24664.29102 to 24253.79688, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 28021.4570 - rmse: 162.1333 - val_loss: 24253.7969 - val_rmse: 190.8640\n",
      "Epoch 127/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 33152.2422 - rmse: 182.0776\n",
      "Epoch 00127: val_loss improved from 24253.79688 to 23737.84570, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 27459.9961 - rmse: 162.8974 - val_loss: 23737.8457 - val_rmse: 188.7534\n",
      "Epoch 128/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 31438.8711 - rmse: 177.3101\n",
      "Epoch 00128: val_loss improved from 23737.84570 to 23381.44727, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 26874.3301 - rmse: 160.4392 - val_loss: 23381.4473 - val_rmse: 187.2291\n",
      "Epoch 129/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 24934.7676 - rmse: 157.9075\n",
      "Epoch 00129: val_loss improved from 23381.44727 to 23259.61328, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 26828.9238 - rmse: 164.1789 - val_loss: 23259.6133 - val_rmse: 186.3476\n",
      "Epoch 130/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 37975.9609 - rmse: 194.8742\n",
      "Epoch 00130: val_loss improved from 23259.61328 to 23238.76367, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 26680.0820 - rmse: 159.2717 - val_loss: 23238.7637 - val_rmse: 185.6864\n",
      "Epoch 131/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 19109.0215 - rmse: 138.2354\n",
      "Epoch 00131: val_loss improved from 23238.76367 to 22570.68359, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 25993.8047 - rmse: 160.6607 - val_loss: 22570.6836 - val_rmse: 183.0697\n",
      "Epoch 132/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 32085.3047 - rmse: 179.1237\n",
      "Epoch 00132: val_loss improved from 22570.68359 to 22552.56250, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 25322.0664 - rmse: 151.8335 - val_loss: 22552.5625 - val_rmse: 183.3524\n",
      "Epoch 133/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 18756.6992 - rmse: 136.9551\n",
      "Epoch 00133: val_loss improved from 22552.56250 to 22119.78320, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 25236.9004 - rmse: 156.8372 - val_loss: 22119.7832 - val_rmse: 181.6737\n",
      "Epoch 134/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 21865.6211 - rmse: 147.8703\n",
      "Epoch 00134: val_loss improved from 22119.78320 to 21781.91992, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 24521.3711 - rmse: 154.7723 - val_loss: 21781.9199 - val_rmse: 180.9706\n",
      "Epoch 135/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 31040.3477 - rmse: 176.1827\n",
      "Epoch 00135: val_loss improved from 21781.91992 to 20864.33398, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 24137.4492 - rmse: 152.0318 - val_loss: 20864.3340 - val_rmse: 176.7629\n",
      "Epoch 136/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 26124.5918 - rmse: 161.6310\n",
      "Epoch 00136: val_loss improved from 20864.33398 to 20804.04688, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 23241.0742 - rmse: 148.9261 - val_loss: 20804.0469 - val_rmse: 176.4991\n",
      "Epoch 137/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 26592.3555 - rmse: 163.0716\n",
      "Epoch 00137: val_loss improved from 20804.04688 to 20433.23633, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 22893.7051 - rmse: 151.8083 - val_loss: 20433.2363 - val_rmse: 174.4237\n",
      "Epoch 138/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 18878.1328 - rmse: 137.3977\n",
      "Epoch 00138: val_loss improved from 20433.23633 to 20083.66406, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 22763.1328 - rmse: 150.0527 - val_loss: 20083.6641 - val_rmse: 173.7164\n",
      "Epoch 139/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 24476.2891 - rmse: 156.4490\n",
      "Epoch 00139: val_loss improved from 20083.66406 to 19366.16992, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 22484.8242 - rmse: 148.3958 - val_loss: 19366.1699 - val_rmse: 170.7124\n",
      "Epoch 140/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 27424.2852 - rmse: 165.6028\n",
      "Epoch 00140: val_loss did not improve from 19366.16992\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 21714.5879 - rmse: 146.5405 - val_loss: 19422.3125 - val_rmse: 171.2522\n",
      "Epoch 141/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 31655.2441 - rmse: 177.9192\n",
      "Epoch 00141: val_loss improved from 19366.16992 to 18769.20898, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 21769.2383 - rmse: 143.3225 - val_loss: 18769.2090 - val_rmse: 168.1922\n",
      "Epoch 142/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 35104.3203 - rmse: 187.3615\n",
      "Epoch 00142: val_loss improved from 18769.20898 to 18565.00977, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 21095.8594 - rmse: 140.9915 - val_loss: 18565.0098 - val_rmse: 167.0586\n",
      "Epoch 143/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 14526.8320 - rmse: 120.5273\n",
      "Epoch 00143: val_loss improved from 18565.00977 to 18249.71094, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 20847.5742 - rmse: 140.9544 - val_loss: 18249.7109 - val_rmse: 165.6216\n",
      "Epoch 144/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 19308.1289 - rmse: 138.9537\n",
      "Epoch 00144: val_loss improved from 18249.71094 to 18117.43164, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 20612.0859 - rmse: 144.3061 - val_loss: 18117.4316 - val_rmse: 165.3242\n",
      "Epoch 145/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 21332.8438 - rmse: 146.0577\n",
      "Epoch 00145: val_loss improved from 18117.43164 to 17708.21094, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 20361.1699 - rmse: 143.7151 - val_loss: 17708.2109 - val_rmse: 163.1601\n",
      "Epoch 146/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 23828.5469 - rmse: 154.3650\n",
      "Epoch 00146: val_loss improved from 17708.21094 to 17466.66211, saving model to weights_rossmann.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 8ms/step - loss: 19984.0020 - rmse: 138.2591 - val_loss: 17466.6621 - val_rmse: 161.9148\n",
      "Epoch 147/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 27102.8633 - rmse: 164.6295\n",
      "Epoch 00147: val_loss improved from 17466.66211 to 17245.98828, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 19601.1035 - rmse: 138.6295 - val_loss: 17245.9883 - val_rmse: 161.0924\n",
      "Epoch 148/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 18068.2793 - rmse: 134.4183\n",
      "Epoch 00148: val_loss improved from 17245.98828 to 17031.22461, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19305.4824 - rmse: 138.3922 - val_loss: 17031.2246 - val_rmse: 159.8837\n",
      "Epoch 149/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 15502.7568 - rmse: 124.5101\n",
      "Epoch 00149: val_loss improved from 17031.22461 to 16811.31641, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 19087.5645 - rmse: 135.7263 - val_loss: 16811.3164 - val_rmse: 158.6308\n",
      "Epoch 150/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 12741.4580 - rmse: 112.8781\n",
      "Epoch 00150: val_loss improved from 16811.31641 to 16551.45703, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 18876.8340 - rmse: 137.5009 - val_loss: 16551.4570 - val_rmse: 157.4939\n",
      "Epoch 151/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 25633.0879 - rmse: 160.1034\n",
      "Epoch 00151: val_loss did not improve from 16551.45703\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 18819.1426 - rmse: 134.3447 - val_loss: 16782.2148 - val_rmse: 158.0053\n",
      "Epoch 152/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9129.1621 - rmse: 95.5466\n",
      "Epoch 00152: val_loss did not improve from 16551.45703\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 18594.8574 - rmse: 135.1594 - val_loss: 16578.7559 - val_rmse: 156.5981\n",
      "Epoch 153/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 17159.2246 - rmse: 130.9932\n",
      "Epoch 00153: val_loss improved from 16551.45703 to 16271.13965, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 18329.4863 - rmse: 136.3523 - val_loss: 16271.1396 - val_rmse: 155.7416\n",
      "Epoch 154/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 22269.7148 - rmse: 149.2304\n",
      "Epoch 00154: val_loss improved from 16271.13965 to 15920.43652, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17824.7520 - rmse: 133.7414 - val_loss: 15920.4365 - val_rmse: 153.9362\n",
      "Epoch 155/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 18596.3984 - rmse: 136.3686\n",
      "Epoch 00155: val_loss improved from 15920.43652 to 15815.40137, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17631.6660 - rmse: 133.4307 - val_loss: 15815.4014 - val_rmse: 153.5239\n",
      "Epoch 156/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9685.4199 - rmse: 98.4145\n",
      "Epoch 00156: val_loss improved from 15815.40137 to 15659.19336, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 17310.8008 - rmse: 128.4744 - val_loss: 15659.1934 - val_rmse: 152.4927\n",
      "Epoch 157/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 22583.3203 - rmse: 150.2775\n",
      "Epoch 00157: val_loss improved from 15659.19336 to 15493.31543, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 17076.8750 - rmse: 128.9074 - val_loss: 15493.3154 - val_rmse: 151.3500\n",
      "Epoch 158/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10247.2949 - rmse: 101.2289\n",
      "Epoch 00158: val_loss improved from 15493.31543 to 15247.70508, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16878.7598 - rmse: 126.7817 - val_loss: 15247.7051 - val_rmse: 150.2480\n",
      "Epoch 159/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 17416.3906 - rmse: 131.9712\n",
      "Epoch 00159: val_loss improved from 15247.70508 to 15103.56348, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16731.0391 - rmse: 126.1333 - val_loss: 15103.5635 - val_rmse: 149.8197\n",
      "Epoch 160/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 21125.4023 - rmse: 145.3458\n",
      "Epoch 00160: val_loss improved from 15103.56348 to 14932.80176, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16480.9805 - rmse: 124.5635 - val_loss: 14932.8018 - val_rmse: 149.0944\n",
      "Epoch 161/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 21834.2520 - rmse: 147.7642\n",
      "Epoch 00161: val_loss improved from 14932.80176 to 14752.84473, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 16270.3799 - rmse: 124.0764 - val_loss: 14752.8447 - val_rmse: 147.6333\n",
      "Epoch 162/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7778.5015 - rmse: 88.1958\n",
      "Epoch 00162: val_loss improved from 14752.84473 to 14597.46289, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 16008.2285 - rmse: 125.6614 - val_loss: 14597.4629 - val_rmse: 146.6489\n",
      "Epoch 163/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 19753.5781 - rmse: 140.5474\n",
      "Epoch 00163: val_loss improved from 14597.46289 to 14500.90625, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 15888.0352 - rmse: 125.9539 - val_loss: 14500.9062 - val_rmse: 145.8757\n",
      "Epoch 164/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 13804.1895 - rmse: 117.4912\n",
      "Epoch 00164: val_loss did not improve from 14500.90625\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 15782.5664 - rmse: 124.2132 - val_loss: 14638.0703 - val_rmse: 146.4443\n",
      "Epoch 165/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 20731.6562 - rmse: 143.9849\n",
      "Epoch 00165: val_loss did not improve from 14500.90625\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 15634.8135 - rmse: 123.7995 - val_loss: 14541.7871 - val_rmse: 145.0377\n",
      "Epoch 166/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 17393.0391 - rmse: 131.8827\n",
      "Epoch 00166: val_loss improved from 14500.90625 to 14400.78027, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15416.0781 - rmse: 123.9825 - val_loss: 14400.7803 - val_rmse: 145.2111\n",
      "Epoch 167/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 16456.8750 - rmse: 128.2843\n",
      "Epoch 00167: val_loss improved from 14400.78027 to 14166.36133, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 15182.4004 - rmse: 123.1234 - val_loss: 14166.3613 - val_rmse: 143.5228\n",
      "Epoch 168/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11513.8867 - rmse: 107.3028\n",
      "Epoch 00168: val_loss improved from 14166.36133 to 14018.54199, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 14905.1885 - rmse: 119.1856 - val_loss: 14018.5420 - val_rmse: 142.9869\n",
      "Epoch 169/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6928.3765 - rmse: 83.2369\n",
      "Epoch 00169: val_loss improved from 14018.54199 to 13846.28711, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 14733.1387 - rmse: 119.3815 - val_loss: 13846.2871 - val_rmse: 141.8584\n",
      "Epoch 170/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 14106.4883 - rmse: 118.7707\n",
      "Epoch 00170: val_loss improved from 13846.28711 to 13729.69336, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 14658.7881 - rmse: 118.6733 - val_loss: 13729.6934 - val_rmse: 141.4834\n",
      "Epoch 171/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 23369.5195 - rmse: 152.8709\n",
      "Epoch 00171: val_loss improved from 13729.69336 to 13714.34180, saving model to weights_rossmann.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 12ms/step - loss: 14511.9629 - rmse: 118.0478 - val_loss: 13714.3418 - val_rmse: 140.8314\n",
      "Epoch 172/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10425.1914 - rmse: 102.1038\n",
      "Epoch 00172: val_loss did not improve from 13714.34180\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 14330.4004 - rmse: 115.4353 - val_loss: 13719.2930 - val_rmse: 140.1158\n",
      "Epoch 173/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6934.4717 - rmse: 83.2735\n",
      "Epoch 00173: val_loss improved from 13714.34180 to 13472.61035, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 14192.3281 - rmse: 118.7459 - val_loss: 13472.6104 - val_rmse: 139.3908\n",
      "Epoch 174/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 15152.0225 - rmse: 123.0936\n",
      "Epoch 00174: val_loss improved from 13472.61035 to 13432.21387, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 14036.3066 - rmse: 118.3007 - val_loss: 13432.2139 - val_rmse: 139.0333\n",
      "Epoch 175/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10165.8604 - rmse: 100.8259\n",
      "Epoch 00175: val_loss improved from 13432.21387 to 13420.33496, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 13907.4434 - rmse: 118.4865 - val_loss: 13420.3350 - val_rmse: 138.2562\n",
      "Epoch 176/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 28074.2188 - rmse: 167.5536\n",
      "Epoch 00176: val_loss did not improve from 13420.33496\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 13885.7695 - rmse: 113.2373 - val_loss: 13796.2480 - val_rmse: 140.2272\n",
      "Epoch 177/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 26064.0352 - rmse: 161.4436\n",
      "Epoch 00177: val_loss improved from 13420.33496 to 13229.20703, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 13896.3418 - rmse: 113.5565 - val_loss: 13229.2070 - val_rmse: 137.1138\n",
      "Epoch 178/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9630.8184 - rmse: 98.1367\n",
      "Epoch 00178: val_loss improved from 13229.20703 to 13011.89941, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 13805.6689 - rmse: 115.0603 - val_loss: 13011.8994 - val_rmse: 135.9338\n",
      "Epoch 179/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8265.8848 - rmse: 90.9169\n",
      "Epoch 00179: val_loss did not improve from 13011.89941\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 13728.5459 - rmse: 116.1903 - val_loss: 13322.0586 - val_rmse: 137.8837\n",
      "Epoch 180/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 16721.8105 - rmse: 129.3128\n",
      "Epoch 00180: val_loss did not improve from 13011.89941\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 13530.8516 - rmse: 116.1046 - val_loss: 13058.1104 - val_rmse: 135.2482\n",
      "Epoch 181/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 16879.5762 - rmse: 129.9214\n",
      "Epoch 00181: val_loss improved from 13011.89941 to 12604.17090, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 13545.5801 - rmse: 111.6263 - val_loss: 12604.1709 - val_rmse: 134.2615\n",
      "Epoch 182/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10313.5674 - rmse: 101.5557\n",
      "Epoch 00182: val_loss did not improve from 12604.17090\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 13015.9775 - rmse: 114.7244 - val_loss: 12620.1816 - val_rmse: 134.1310\n",
      "Epoch 183/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11521.6445 - rmse: 107.3389\n",
      "Epoch 00183: val_loss did not improve from 12604.17090\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 12875.3389 - rmse: 109.7018 - val_loss: 12764.5547 - val_rmse: 133.6298\n",
      "Epoch 184/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 13275.5791 - rmse: 115.2197\n",
      "Epoch 00184: val_loss improved from 12604.17090 to 12518.73340, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 12917.8467 - rmse: 113.9962 - val_loss: 12518.7334 - val_rmse: 133.4282\n",
      "Epoch 185/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 15238.6221 - rmse: 123.4448\n",
      "Epoch 00185: val_loss improved from 12518.73340 to 12243.29297, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 12788.9443 - rmse: 112.9888 - val_loss: 12243.2930 - val_rmse: 131.7869\n",
      "Epoch 186/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 16352.5029 - rmse: 127.8769\n",
      "Epoch 00186: val_loss improved from 12243.29297 to 12178.56641, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 12442.6084 - rmse: 108.2993 - val_loss: 12178.5664 - val_rmse: 131.1658\n",
      "Epoch 187/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10220.1143 - rmse: 101.0946\n",
      "Epoch 00187: val_loss did not improve from 12178.56641\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12390.1768 - rmse: 107.8173 - val_loss: 12216.5078 - val_rmse: 131.9122\n",
      "Epoch 188/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 23804.0156 - rmse: 154.2855\n",
      "Epoch 00188: val_loss improved from 12178.56641 to 12062.27930, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 12360.2861 - rmse: 107.2993 - val_loss: 12062.2793 - val_rmse: 130.2780\n",
      "Epoch 189/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 17979.1191 - rmse: 134.0862\n",
      "Epoch 00189: val_loss improved from 12062.27930 to 11997.50781, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 12055.0928 - rmse: 105.8959 - val_loss: 11997.5078 - val_rmse: 129.7328\n",
      "Epoch 190/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 14431.2383 - rmse: 120.1301\n",
      "Epoch 00190: val_loss improved from 11997.50781 to 11824.36523, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11921.7305 - rmse: 106.9853 - val_loss: 11824.3652 - val_rmse: 129.1029\n",
      "Epoch 191/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9466.2109 - rmse: 97.2945\n",
      "Epoch 00191: val_loss improved from 11824.36523 to 11729.22363, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11832.3779 - rmse: 108.7742 - val_loss: 11729.2236 - val_rmse: 128.2926\n",
      "Epoch 192/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7955.0068 - rmse: 89.1908\n",
      "Epoch 00192: val_loss did not improve from 11729.22363\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 11751.1416 - rmse: 105.2434 - val_loss: 11850.2354 - val_rmse: 128.8927\n",
      "Epoch 193/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7128.0127 - rmse: 84.4276\n",
      "Epoch 00193: val_loss did not improve from 11729.22363\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 11737.0801 - rmse: 105.0496 - val_loss: 12003.3564 - val_rmse: 128.2166\n",
      "Epoch 194/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 13432.0566 - rmse: 115.8968\n",
      "Epoch 00194: val_loss did not improve from 11729.22363\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 11648.5713 - rmse: 107.1098 - val_loss: 11758.6035 - val_rmse: 128.1022\n",
      "Epoch 195/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 16141.0254 - rmse: 127.0473\n",
      "Epoch 00195: val_loss improved from 11729.22363 to 11703.34473, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 11753.5215 - rmse: 106.2654 - val_loss: 11703.3447 - val_rmse: 127.4785\n",
      "Epoch 196/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 20271.3359 - rmse: 142.3774\n",
      "Epoch 00196: val_loss improved from 11703.34473 to 11693.27539, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 11473.3672 - rmse: 103.3418 - val_loss: 11693.2754 - val_rmse: 126.8355\n",
      "Epoch 197/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9095.4346 - rmse: 95.3700\n",
      "Epoch 00197: val_loss improved from 11693.27539 to 11564.59375, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 11264.3223 - rmse: 101.9196 - val_loss: 11564.5938 - val_rmse: 125.9973\n",
      "Epoch 198/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 14868.7559 - rmse: 121.9375\n",
      "Epoch 00198: val_loss improved from 11564.59375 to 11459.17871, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 11147.8213 - rmse: 103.4954 - val_loss: 11459.1787 - val_rmse: 125.4189\n",
      "Epoch 199/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 14171.8154 - rmse: 119.0454\n",
      "Epoch 00199: val_loss improved from 11459.17871 to 11426.27832, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 11040.7432 - rmse: 102.9513 - val_loss: 11426.2783 - val_rmse: 125.3000\n",
      "Epoch 200/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10436.7598 - rmse: 102.1605\n",
      "Epoch 00200: val_loss did not improve from 11426.27832\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 10960.7412 - rmse: 103.0163 - val_loss: 11438.0605 - val_rmse: 124.8280\n",
      "Epoch 201/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 12151.4258 - rmse: 110.2335\n",
      "Epoch 00201: val_loss improved from 11426.27832 to 11411.42480, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 10999.4434 - rmse: 102.5335 - val_loss: 11411.4248 - val_rmse: 124.9241\n",
      "Epoch 202/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7674.1748 - rmse: 87.6024\n",
      "Epoch 00202: val_loss improved from 11411.42480 to 11280.64355, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 10858.4326 - rmse: 100.6688 - val_loss: 11280.6436 - val_rmse: 123.7382\n",
      "Epoch 203/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 13141.8164 - rmse: 114.6378\n",
      "Epoch 00203: val_loss did not improve from 11280.64355\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 10709.2812 - rmse: 102.7143 - val_loss: 11313.4385 - val_rmse: 123.3826\n",
      "Epoch 204/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8573.7051 - rmse: 92.5943\n",
      "Epoch 00204: val_loss improved from 11280.64355 to 11263.71484, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 10644.0293 - rmse: 100.5646 - val_loss: 11263.7148 - val_rmse: 123.6238\n",
      "Epoch 205/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7900.6104 - rmse: 88.8854\n",
      "Epoch 00205: val_loss did not improve from 11263.71484\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 10570.1055 - rmse: 99.8068 - val_loss: 11310.6562 - val_rmse: 122.9930\n",
      "Epoch 206/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11668.1328 - rmse: 108.0191\n",
      "Epoch 00206: val_loss improved from 11263.71484 to 11194.56836, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 10461.1221 - rmse: 102.6866 - val_loss: 11194.5684 - val_rmse: 123.1316\n",
      "Epoch 207/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 12409.8076 - rmse: 111.3993\n",
      "Epoch 00207: val_loss improved from 11194.56836 to 11095.23633, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 10413.7656 - rmse: 99.1111 - val_loss: 11095.2363 - val_rmse: 122.1931\n",
      "Epoch 208/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10289.8828 - rmse: 101.4391\n",
      "Epoch 00208: val_loss improved from 11095.23633 to 11058.23828, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 10307.2188 - rmse: 98.7544 - val_loss: 11058.2383 - val_rmse: 121.7743\n",
      "Epoch 209/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6910.8716 - rmse: 83.1317\n",
      "Epoch 00209: val_loss did not improve from 11058.23828\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 10233.0654 - rmse: 98.8031 - val_loss: 11096.2881 - val_rmse: 122.3977\n",
      "Epoch 210/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 16871.7871 - rmse: 129.8914\n",
      "Epoch 00210: val_loss did not improve from 11058.23828\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 10199.3203 - rmse: 99.0767 - val_loss: 11068.4854 - val_rmse: 121.0286\n",
      "Epoch 211/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6548.8428 - rmse: 80.9249\n",
      "Epoch 00211: val_loss improved from 11058.23828 to 10899.20215, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 10086.7695 - rmse: 100.3516 - val_loss: 10899.2021 - val_rmse: 121.0583\n",
      "Epoch 212/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10145.6211 - rmse: 100.7255\n",
      "Epoch 00212: val_loss improved from 10899.20215 to 10826.42090, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 10152.5635 - rmse: 96.3169 - val_loss: 10826.4209 - val_rmse: 120.0506\n",
      "Epoch 213/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11455.0410 - rmse: 107.0282\n",
      "Epoch 00213: val_loss did not improve from 10826.42090\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 10038.8926 - rmse: 97.6400 - val_loss: 10867.5000 - val_rmse: 119.8688\n",
      "Epoch 214/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6444.4243 - rmse: 80.2772\n",
      "Epoch 00214: val_loss improved from 10826.42090 to 10718.53711, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9865.8945 - rmse: 98.8705 - val_loss: 10718.5371 - val_rmse: 119.6956\n",
      "Epoch 215/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 12832.5020 - rmse: 113.2806\n",
      "Epoch 00215: val_loss improved from 10718.53711 to 10607.04492, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9777.5088 - rmse: 96.5700 - val_loss: 10607.0449 - val_rmse: 118.4377\n",
      "Epoch 216/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3723.9429 - rmse: 61.0241\n",
      "Epoch 00216: val_loss did not improve from 10607.04492\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 9713.7529 - rmse: 92.6681 - val_loss: 10646.6650 - val_rmse: 118.4846\n",
      "Epoch 217/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 13912.9023 - rmse: 117.9530\n",
      "Epoch 00217: val_loss improved from 10607.04492 to 10590.41113, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9617.7432 - rmse: 95.4545 - val_loss: 10590.4111 - val_rmse: 118.3134\n",
      "Epoch 218/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 13129.2539 - rmse: 114.5830\n",
      "Epoch 00218: val_loss improved from 10590.41113 to 10572.75293, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9589.1309 - rmse: 96.6844 - val_loss: 10572.7529 - val_rmse: 118.4883\n",
      "Epoch 219/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 14081.6445 - rmse: 118.6661\n",
      "Epoch 00219: val_loss improved from 10572.75293 to 10495.25977, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9583.7852 - rmse: 94.6981 - val_loss: 10495.2598 - val_rmse: 118.0615\n",
      "Epoch 220/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6445.3604 - rmse: 80.2830\n",
      "Epoch 00220: val_loss improved from 10495.25977 to 10475.14160, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 9521.7930 - rmse: 93.4223 - val_loss: 10475.1416 - val_rmse: 117.0803\n",
      "Epoch 221/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9105.5312 - rmse: 95.4229\n",
      "Epoch 00221: val_loss improved from 10475.14160 to 10413.53613, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9339.3359 - rmse: 94.9566 - val_loss: 10413.5361 - val_rmse: 117.0125\n",
      "Epoch 222/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11895.5000 - rmse: 109.0665\n",
      "Epoch 00222: val_loss did not improve from 10413.53613\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 9292.0430 - rmse: 94.6512 - val_loss: 10495.5098 - val_rmse: 116.6265\n",
      "Epoch 223/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5 [=====>........................] - ETA: 0s - loss: 9461.1475 - rmse: 97.2684\n",
      "Epoch 00223: val_loss did not improve from 10413.53613\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 9267.3770 - rmse: 92.9369 - val_loss: 10493.8115 - val_rmse: 117.4501\n",
      "Epoch 224/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5752.6357 - rmse: 75.8461\n",
      "Epoch 00224: val_loss improved from 10413.53613 to 10382.58887, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 9290.3984 - rmse: 91.9337 - val_loss: 10382.5889 - val_rmse: 116.0947\n",
      "Epoch 225/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3345.7695 - rmse: 57.8426\n",
      "Epoch 00225: val_loss did not improve from 10382.58887\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 9294.0371 - rmse: 94.5787 - val_loss: 10584.6836 - val_rmse: 116.1569\n",
      "Epoch 226/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7266.2285 - rmse: 85.2422\n",
      "Epoch 00226: val_loss did not improve from 10382.58887\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 9263.1074 - rmse: 94.4234 - val_loss: 10657.2832 - val_rmse: 117.7126\n",
      "Epoch 227/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5188.9146 - rmse: 72.0341\n",
      "Epoch 00227: val_loss did not improve from 10382.58887\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 9120.0957 - rmse: 95.5612 - val_loss: 10635.7695 - val_rmse: 115.9837\n",
      "Epoch 228/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7186.7363 - rmse: 84.7746\n",
      "Epoch 00228: val_loss improved from 10382.58887 to 10307.86035, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 9159.7197 - rmse: 95.6742 - val_loss: 10307.8604 - val_rmse: 115.5786\n",
      "Epoch 229/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4040.5378 - rmse: 63.5652\n",
      "Epoch 00229: val_loss improved from 10307.86035 to 10290.47461, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 8883.0293 - rmse: 91.9758 - val_loss: 10290.4746 - val_rmse: 115.0572\n",
      "Epoch 230/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6376.0518 - rmse: 79.8502\n",
      "Epoch 00230: val_loss did not improve from 10290.47461\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 9039.5195 - rmse: 93.9163 - val_loss: 10502.2930 - val_rmse: 115.2996\n",
      "Epoch 231/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 13935.6025 - rmse: 118.0492\n",
      "Epoch 00231: val_loss improved from 10290.47461 to 10243.37695, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 8940.2812 - rmse: 93.5969 - val_loss: 10243.3770 - val_rmse: 114.9943\n",
      "Epoch 232/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 13963.4727 - rmse: 118.1671\n",
      "Epoch 00232: val_loss improved from 10243.37695 to 10123.32031, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 8791.2900 - rmse: 91.5060 - val_loss: 10123.3203 - val_rmse: 113.5759\n",
      "Epoch 233/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6744.5229 - rmse: 82.1250\n",
      "Epoch 00233: val_loss did not improve from 10123.32031\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 8785.3896 - rmse: 90.5561 - val_loss: 10204.4463 - val_rmse: 113.4295\n",
      "Epoch 234/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5653.5347 - rmse: 75.1900\n",
      "Epoch 00234: val_loss improved from 10123.32031 to 10017.07227, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 8694.5439 - rmse: 93.7631 - val_loss: 10017.0723 - val_rmse: 113.1033\n",
      "Epoch 235/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3951.5464 - rmse: 62.8613\n",
      "Epoch 00235: val_loss did not improve from 10017.07227\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 8604.5732 - rmse: 89.4968 - val_loss: 10047.2051 - val_rmse: 113.0313\n",
      "Epoch 236/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8403.4238 - rmse: 91.6702\n",
      "Epoch 00236: val_loss did not improve from 10017.07227\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 8572.5029 - rmse: 91.8425 - val_loss: 10182.3271 - val_rmse: 113.0834\n",
      "Epoch 237/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6596.2021 - rmse: 81.2170\n",
      "Epoch 00237: val_loss did not improve from 10017.07227\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 8449.6162 - rmse: 91.0001 - val_loss: 10077.9316 - val_rmse: 112.3328\n",
      "Epoch 238/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9709.1523 - rmse: 98.5350\n",
      "Epoch 00238: val_loss improved from 10017.07227 to 9977.34473, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 8375.7070 - rmse: 87.4352 - val_loss: 9977.3447 - val_rmse: 112.3951\n",
      "Epoch 239/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9895.6133 - rmse: 99.4767\n",
      "Epoch 00239: val_loss did not improve from 9977.34473\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 8332.3223 - rmse: 88.2371 - val_loss: 9991.2197 - val_rmse: 111.7697\n",
      "Epoch 240/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10045.5088 - rmse: 100.2273\n",
      "Epoch 00240: val_loss improved from 9977.34473 to 9837.87402, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 8260.8291 - rmse: 87.8168 - val_loss: 9837.8740 - val_rmse: 111.4708\n",
      "Epoch 241/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 13219.4619 - rmse: 114.9759\n",
      "Epoch 00241: val_loss did not improve from 9837.87402\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 8263.5557 - rmse: 89.9456 - val_loss: 9859.6543 - val_rmse: 111.6513\n",
      "Epoch 242/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6401.0645 - rmse: 80.0067\n",
      "Epoch 00242: val_loss improved from 9837.87402 to 9804.81445, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 8198.2051 - rmse: 87.2671 - val_loss: 9804.8145 - val_rmse: 110.5697\n",
      "Epoch 243/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5816.6768 - rmse: 76.2671\n",
      "Epoch 00243: val_loss improved from 9804.81445 to 9607.82129, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 8148.9849 - rmse: 90.7591 - val_loss: 9607.8213 - val_rmse: 109.8976\n",
      "Epoch 244/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4466.7778 - rmse: 66.8340\n",
      "Epoch 00244: val_loss did not improve from 9607.82129\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 8057.9189 - rmse: 85.4152 - val_loss: 9614.2979 - val_rmse: 109.6431\n",
      "Epoch 245/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11454.4727 - rmse: 107.0256\n",
      "Epoch 00245: val_loss did not improve from 9607.82129\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 8002.7051 - rmse: 87.0955 - val_loss: 9649.0752 - val_rmse: 110.0785\n",
      "Epoch 246/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6357.0596 - rmse: 79.7312\n",
      "Epoch 00246: val_loss did not improve from 9607.82129\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 8065.1367 - rmse: 88.6598 - val_loss: 9757.4033 - val_rmse: 110.7523\n",
      "Epoch 247/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11080.2666 - rmse: 105.2628\n",
      "Epoch 00247: val_loss did not improve from 9607.82129\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 8065.7891 - rmse: 89.4413 - val_loss: 9827.4297 - val_rmse: 109.7080\n",
      "Epoch 248/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10608.1270 - rmse: 102.9958\n",
      "Epoch 00248: val_loss improved from 9607.82129 to 9579.20801, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 7972.7051 - rmse: 85.7790 - val_loss: 9579.2080 - val_rmse: 109.7822\n",
      "Epoch 249/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6142.7217 - rmse: 78.3755\n",
      "Epoch 00249: val_loss improved from 9579.20801 to 9439.54980, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 7913.6636 - rmse: 87.3941 - val_loss: 9439.5498 - val_rmse: 108.5532\n",
      "Epoch 250/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10356.4707 - rmse: 101.7667\n",
      "Epoch 00250: val_loss did not improve from 9439.54980\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 8003.2441 - rmse: 87.9625 - val_loss: 9618.9580 - val_rmse: 108.6353\n",
      "Epoch 251/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3848.1689 - rmse: 62.0336\n",
      "Epoch 00251: val_loss did not improve from 9439.54980\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 7829.9019 - rmse: 86.3789 - val_loss: 9462.3701 - val_rmse: 108.8759\n",
      "Epoch 252/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6024.7783 - rmse: 77.6194\n",
      "Epoch 00252: val_loss improved from 9439.54980 to 9386.41504, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 7776.4082 - rmse: 86.8371 - val_loss: 9386.4150 - val_rmse: 107.9246\n",
      "Epoch 253/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9811.0762 - rmse: 99.0509\n",
      "Epoch 00253: val_loss did not improve from 9386.41504\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 7732.6343 - rmse: 87.0205 - val_loss: 9493.7754 - val_rmse: 107.8985\n",
      "Epoch 254/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5496.5620 - rmse: 74.1388\n",
      "Epoch 00254: val_loss improved from 9386.41504 to 9367.92969, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 7648.7183 - rmse: 85.4330 - val_loss: 9367.9297 - val_rmse: 107.5594\n",
      "Epoch 255/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4266.7471 - rmse: 65.3203\n",
      "Epoch 00255: val_loss improved from 9367.92969 to 9287.16211, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 7534.5498 - rmse: 86.4812 - val_loss: 9287.1621 - val_rmse: 106.7576\n",
      "Epoch 256/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 15006.8730 - rmse: 122.5025\n",
      "Epoch 00256: val_loss did not improve from 9287.16211\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 7537.8208 - rmse: 83.8563 - val_loss: 9342.0850 - val_rmse: 107.5295\n",
      "Epoch 257/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5610.2080 - rmse: 74.9013\n",
      "Epoch 00257: val_loss did not improve from 9287.16211\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 7476.2686 - rmse: 86.3071 - val_loss: 9341.8838 - val_rmse: 106.6932\n",
      "Epoch 258/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8130.2510 - rmse: 90.1679\n",
      "Epoch 00258: val_loss improved from 9287.16211 to 9213.37207, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 7480.7285 - rmse: 82.0220 - val_loss: 9213.3721 - val_rmse: 106.3332\n",
      "Epoch 259/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8763.9707 - rmse: 93.6161\n",
      "Epoch 00259: val_loss improved from 9213.37207 to 9163.78516, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 7387.7065 - rmse: 84.0924 - val_loss: 9163.7852 - val_rmse: 106.0710\n",
      "Epoch 260/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9293.6396 - rmse: 96.4035\n",
      "Epoch 00260: val_loss improved from 9163.78516 to 9127.11621, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 7328.7266 - rmse: 83.4242 - val_loss: 9127.1162 - val_rmse: 105.8952\n",
      "Epoch 261/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6738.8564 - rmse: 82.0905\n",
      "Epoch 00261: val_loss improved from 9127.11621 to 9063.79688, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 7306.2686 - rmse: 82.0951 - val_loss: 9063.7969 - val_rmse: 105.1775\n",
      "Epoch 262/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6405.6719 - rmse: 80.0354\n",
      "Epoch 00262: val_loss improved from 9063.79688 to 9025.93457, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 7277.2798 - rmse: 85.2091 - val_loss: 9025.9346 - val_rmse: 105.2715\n",
      "Epoch 263/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10044.6738 - rmse: 100.2231\n",
      "Epoch 00263: val_loss did not improve from 9025.93457\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 7244.1011 - rmse: 84.0971 - val_loss: 9038.0811 - val_rmse: 104.7490\n",
      "Epoch 264/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6400.4844 - rmse: 80.0030\n",
      "Epoch 00264: val_loss improved from 9025.93457 to 9008.45508, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 7249.2334 - rmse: 85.7529 - val_loss: 9008.4551 - val_rmse: 104.4763\n",
      "Epoch 265/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4809.6636 - rmse: 69.3517\n",
      "Epoch 00265: val_loss did not improve from 9008.45508\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 7322.6382 - rmse: 84.6959 - val_loss: 9053.5947 - val_rmse: 105.4941\n",
      "Epoch 266/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 12513.2871 - rmse: 111.8628\n",
      "Epoch 00266: val_loss improved from 9008.45508 to 8949.66406, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 7361.2124 - rmse: 83.0382 - val_loss: 8949.6641 - val_rmse: 104.4886\n",
      "Epoch 267/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8609.3506 - rmse: 92.7866\n",
      "Epoch 00267: val_loss did not improve from 8949.66406\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 7256.1426 - rmse: 85.3376 - val_loss: 8977.1846 - val_rmse: 104.0214\n",
      "Epoch 268/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11566.0801 - rmse: 107.5457\n",
      "Epoch 00268: val_loss did not improve from 8949.66406\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 7105.6582 - rmse: 83.1752 - val_loss: 9207.9902 - val_rmse: 106.5646\n",
      "Epoch 269/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9568.1953 - rmse: 97.8172\n",
      "Epoch 00269: val_loss improved from 8949.66406 to 8801.44043, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 7358.7773 - rmse: 83.9056 - val_loss: 8801.4404 - val_rmse: 103.1084\n",
      "Epoch 270/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 13243.7100 - rmse: 115.0813\n",
      "Epoch 00270: val_loss improved from 8801.44043 to 8669.79785, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 7042.5508 - rmse: 81.0115 - val_loss: 8669.7979 - val_rmse: 102.9573\n",
      "Epoch 271/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8592.3838 - rmse: 92.6951\n",
      "Epoch 00271: val_loss did not improve from 8669.79785\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6988.7988 - rmse: 81.5176 - val_loss: 8683.2979 - val_rmse: 103.3158\n",
      "Epoch 272/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5820.1255 - rmse: 76.2897\n",
      "Epoch 00272: val_loss improved from 8669.79785 to 8628.87207, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6952.0176 - rmse: 83.0575 - val_loss: 8628.8721 - val_rmse: 102.1046\n",
      "Epoch 273/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9811.8770 - rmse: 99.0549\n",
      "Epoch 00273: val_loss improved from 8628.87207 to 8576.13672, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 6944.2441 - rmse: 82.0123 - val_loss: 8576.1367 - val_rmse: 102.2523\n",
      "Epoch 274/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11992.4961 - rmse: 109.5103\n",
      "Epoch 00274: val_loss did not improve from 8576.13672\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6929.2222 - rmse: 82.0199 - val_loss: 8653.6934 - val_rmse: 102.7169\n",
      "Epoch 275/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5750.9707 - rmse: 75.8352\n",
      "Epoch 00275: val_loss did not improve from 8576.13672\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 6883.5664 - rmse: 83.4430 - val_loss: 8624.3691 - val_rmse: 101.6470\n",
      "Epoch 276/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4651.1182 - rmse: 68.1991\n",
      "Epoch 00276: val_loss improved from 8576.13672 to 8481.00879, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 6825.5156 - rmse: 83.1374 - val_loss: 8481.0088 - val_rmse: 101.4082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 277/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4459.5996 - rmse: 66.7802\n",
      "Epoch 00277: val_loss did not improve from 8481.00879\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 6760.3516 - rmse: 80.6878 - val_loss: 8550.1982 - val_rmse: 101.7925\n",
      "Epoch 278/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4923.2129 - rmse: 70.1656\n",
      "Epoch 00278: val_loss did not improve from 8481.00879\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 6721.8765 - rmse: 82.1124 - val_loss: 8676.0635 - val_rmse: 101.5451\n",
      "Epoch 279/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5030.5747 - rmse: 70.9265\n",
      "Epoch 00279: val_loss did not improve from 8481.00879\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6728.4888 - rmse: 80.4225 - val_loss: 8733.2568 - val_rmse: 102.9866\n",
      "Epoch 280/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7856.7236 - rmse: 88.6382\n",
      "Epoch 00280: val_loss did not improve from 8481.00879\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 6747.5518 - rmse: 81.3241 - val_loss: 8586.1221 - val_rmse: 100.9772\n",
      "Epoch 281/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3828.3501 - rmse: 61.8737\n",
      "Epoch 00281: val_loss did not improve from 8481.00879\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 6670.2373 - rmse: 78.1809 - val_loss: 8600.2412 - val_rmse: 100.8916\n",
      "Epoch 282/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 15016.3477 - rmse: 122.5412\n",
      "Epoch 00282: val_loss did not improve from 8481.00879\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6699.2485 - rmse: 77.7575 - val_loss: 8733.2217 - val_rmse: 102.3697\n",
      "Epoch 283/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5174.7466 - rmse: 71.9357\n",
      "Epoch 00283: val_loss did not improve from 8481.00879\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 6625.2910 - rmse: 80.1838 - val_loss: 8614.9746 - val_rmse: 100.7347\n",
      "Epoch 284/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9683.7695 - rmse: 98.4061\n",
      "Epoch 00284: val_loss improved from 8481.00879 to 8386.06250, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 6564.1689 - rmse: 78.6559 - val_loss: 8386.0625 - val_rmse: 100.3743\n",
      "Epoch 285/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4199.7427 - rmse: 64.8054\n",
      "Epoch 00285: val_loss improved from 8386.06250 to 8359.71484, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 6504.3184 - rmse: 80.8376 - val_loss: 8359.7148 - val_rmse: 100.1602\n",
      "Epoch 286/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3066.1333 - rmse: 55.3727\n",
      "Epoch 00286: val_loss did not improve from 8359.71484\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6460.8677 - rmse: 76.9574 - val_loss: 8376.6885 - val_rmse: 99.4841\n",
      "Epoch 287/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11579.8838 - rmse: 107.6099\n",
      "Epoch 00287: val_loss did not improve from 8359.71484\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6395.6074 - rmse: 76.1843 - val_loss: 8431.2480 - val_rmse: 100.7262\n",
      "Epoch 288/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6803.8662 - rmse: 82.4855\n",
      "Epoch 00288: val_loss improved from 8359.71484 to 8239.47559, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 6431.2734 - rmse: 79.5338 - val_loss: 8239.4756 - val_rmse: 98.8676\n",
      "Epoch 289/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9224.1035 - rmse: 96.0422\n",
      "Epoch 00289: val_loss improved from 8239.47559 to 8171.91504, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 6327.4355 - rmse: 79.0205 - val_loss: 8171.9150 - val_rmse: 99.0436\n",
      "Epoch 290/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4365.2329 - rmse: 66.0699\n",
      "Epoch 00290: val_loss did not improve from 8171.91504\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 6455.6479 - rmse: 79.4890 - val_loss: 8278.6641 - val_rmse: 99.7070\n",
      "Epoch 291/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6217.9419 - rmse: 78.8539\n",
      "Epoch 00291: val_loss did not improve from 8171.91504\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 6370.5654 - rmse: 78.2384 - val_loss: 8442.2979 - val_rmse: 99.2290\n",
      "Epoch 292/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5149.6265 - rmse: 71.7609\n",
      "Epoch 00292: val_loss did not improve from 8171.91504\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6296.3589 - rmse: 76.2684 - val_loss: 8184.8584 - val_rmse: 98.9606\n",
      "Epoch 293/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8457.8809 - rmse: 91.9667\n",
      "Epoch 00293: val_loss improved from 8171.91504 to 8071.92236, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 6213.5068 - rmse: 78.1584 - val_loss: 8071.9224 - val_rmse: 97.9197\n",
      "Epoch 294/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6841.6035 - rmse: 82.7140\n",
      "Epoch 00294: val_loss improved from 8071.92236 to 8037.85840, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 6160.4722 - rmse: 76.4324 - val_loss: 8037.8584 - val_rmse: 98.1665\n",
      "Epoch 295/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8119.7202 - rmse: 90.1095\n",
      "Epoch 00295: val_loss improved from 8037.85840 to 7985.32422, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 6165.6040 - rmse: 78.7356 - val_loss: 7985.3242 - val_rmse: 97.4184\n",
      "Epoch 296/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11610.6016 - rmse: 107.7525\n",
      "Epoch 00296: val_loss did not improve from 7985.32422\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6130.2432 - rmse: 73.7023 - val_loss: 8041.8159 - val_rmse: 97.9559\n",
      "Epoch 297/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4587.4961 - rmse: 67.7311\n",
      "Epoch 00297: val_loss did not improve from 7985.32422\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 6156.5386 - rmse: 77.5054 - val_loss: 8005.2207 - val_rmse: 96.9489\n",
      "Epoch 298/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 11207.7539 - rmse: 105.8667\n",
      "Epoch 00298: val_loss improved from 7985.32422 to 7974.86182, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 6146.2666 - rmse: 76.4318 - val_loss: 7974.8618 - val_rmse: 97.1857\n",
      "Epoch 299/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6678.2432 - rmse: 81.7205\n",
      "Epoch 00299: val_loss did not improve from 7974.86182\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 6064.5435 - rmse: 75.6631 - val_loss: 8021.9282 - val_rmse: 97.4831\n",
      "Epoch 300/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7259.2197 - rmse: 85.2011\n",
      "Epoch 00300: val_loss did not improve from 7974.86182\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 6050.2397 - rmse: 76.6929 - val_loss: 8047.6851 - val_rmse: 97.5137\n",
      "Epoch 301/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4879.5703 - rmse: 69.8539\n",
      "Epoch 00301: val_loss did not improve from 7974.86182\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5993.7749 - rmse: 77.5357 - val_loss: 7978.2500 - val_rmse: 96.6086\n",
      "Epoch 302/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3442.6943 - rmse: 58.6745\n",
      "Epoch 00302: val_loss improved from 7974.86182 to 7888.39160, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 5991.7388 - rmse: 75.5757 - val_loss: 7888.3916 - val_rmse: 96.3169\n",
      "Epoch 303/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4644.0073 - rmse: 68.1470\n",
      "Epoch 00303: val_loss improved from 7888.39160 to 7860.23486, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5934.0269 - rmse: 74.5497 - val_loss: 7860.2349 - val_rmse: 96.3474\n",
      "Epoch 304/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7987.8994 - rmse: 89.3750\n",
      "Epoch 00304: val_loss did not improve from 7860.23486\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 5921.2153 - rmse: 73.4226 - val_loss: 7914.1909 - val_rmse: 96.1485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 305/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4198.2676 - rmse: 64.7940\n",
      "Epoch 00305: val_loss improved from 7860.23486 to 7786.57227, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5897.9043 - rmse: 76.3896 - val_loss: 7786.5723 - val_rmse: 95.6076\n",
      "Epoch 306/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3277.8435 - rmse: 57.2525\n",
      "Epoch 00306: val_loss improved from 7786.57227 to 7730.73145, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5850.1177 - rmse: 75.7058 - val_loss: 7730.7314 - val_rmse: 95.4209\n",
      "Epoch 307/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7708.0464 - rmse: 87.7955\n",
      "Epoch 00307: val_loss improved from 7730.73145 to 7688.53320, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5806.0449 - rmse: 73.0989 - val_loss: 7688.5332 - val_rmse: 95.0547\n",
      "Epoch 308/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3946.1433 - rmse: 62.8183\n",
      "Epoch 00308: val_loss did not improve from 7688.53320\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 5820.2998 - rmse: 76.4413 - val_loss: 7712.5601 - val_rmse: 94.8166\n",
      "Epoch 309/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3135.8276 - rmse: 55.9985\n",
      "Epoch 00309: val_loss improved from 7688.53320 to 7682.01416, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 5764.3838 - rmse: 74.0650 - val_loss: 7682.0142 - val_rmse: 95.3416\n",
      "Epoch 310/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5947.6025 - rmse: 77.1207\n",
      "Epoch 00310: val_loss improved from 7682.01416 to 7602.42236, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 5814.7378 - rmse: 74.0817 - val_loss: 7602.4224 - val_rmse: 94.5907\n",
      "Epoch 311/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5389.1616 - rmse: 73.4109\n",
      "Epoch 00311: val_loss did not improve from 7602.42236\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 5720.9888 - rmse: 73.9889 - val_loss: 7618.0459 - val_rmse: 94.5037\n",
      "Epoch 312/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7774.6289 - rmse: 88.1739\n",
      "Epoch 00312: val_loss did not improve from 7602.42236\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5684.9570 - rmse: 71.7032 - val_loss: 7622.8594 - val_rmse: 94.8716\n",
      "Epoch 313/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10119.5010 - rmse: 100.5957\n",
      "Epoch 00313: val_loss did not improve from 7602.42236\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5696.3652 - rmse: 73.2127 - val_loss: 7650.7617 - val_rmse: 94.1697\n",
      "Epoch 314/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 10385.9238 - rmse: 101.9114\n",
      "Epoch 00314: val_loss improved from 7602.42236 to 7586.62646, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 5645.2827 - rmse: 72.0536 - val_loss: 7586.6265 - val_rmse: 93.9060\n",
      "Epoch 315/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3257.2217 - rmse: 57.0721\n",
      "Epoch 00315: val_loss improved from 7586.62646 to 7462.89648, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 5629.4248 - rmse: 73.9792 - val_loss: 7462.8965 - val_rmse: 93.7541\n",
      "Epoch 316/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3493.2200 - rmse: 59.1035\n",
      "Epoch 00316: val_loss did not improve from 7462.89648\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 5618.8140 - rmse: 74.3384 - val_loss: 7496.7524 - val_rmse: 93.3696\n",
      "Epoch 317/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5689.9731 - rmse: 75.4319\n",
      "Epoch 00317: val_loss did not improve from 7462.89648\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5592.6650 - rmse: 74.3402 - val_loss: 7587.5718 - val_rmse: 94.3447\n",
      "Epoch 318/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6335.5317 - rmse: 79.5961\n",
      "Epoch 00318: val_loss did not improve from 7462.89648\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5544.6973 - rmse: 74.4517 - val_loss: 7614.3765 - val_rmse: 93.7712\n",
      "Epoch 319/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4532.6450 - rmse: 67.3249\n",
      "Epoch 00319: val_loss did not improve from 7462.89648\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 5519.2251 - rmse: 72.5606 - val_loss: 7559.6440 - val_rmse: 94.0248\n",
      "Epoch 320/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7104.4766 - rmse: 84.2881\n",
      "Epoch 00320: val_loss improved from 7462.89648 to 7442.94678, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5581.6963 - rmse: 72.6453 - val_loss: 7442.9468 - val_rmse: 93.0144\n",
      "Epoch 321/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 9378.3525 - rmse: 96.8419\n",
      "Epoch 00321: val_loss improved from 7442.94678 to 7314.35938, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5479.3877 - rmse: 70.9307 - val_loss: 7314.3594 - val_rmse: 92.4197\n",
      "Epoch 322/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3913.3892 - rmse: 62.5571\n",
      "Epoch 00322: val_loss improved from 7314.35938 to 7286.27979, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5456.8638 - rmse: 69.3986 - val_loss: 7286.2798 - val_rmse: 92.1076\n",
      "Epoch 323/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4386.6831 - rmse: 66.2320\n",
      "Epoch 00323: val_loss did not improve from 7286.27979\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 5471.3101 - rmse: 74.5110 - val_loss: 7331.1851 - val_rmse: 91.8979\n",
      "Epoch 324/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3033.5955 - rmse: 55.0781\n",
      "Epoch 00324: val_loss did not improve from 7286.27979\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5422.3086 - rmse: 70.5231 - val_loss: 7301.1758 - val_rmse: 92.0816\n",
      "Epoch 325/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5393.9775 - rmse: 73.4437\n",
      "Epoch 00325: val_loss did not improve from 7286.27979\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 5379.9302 - rmse: 71.1817 - val_loss: 7288.4341 - val_rmse: 92.0252\n",
      "Epoch 326/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6682.2803 - rmse: 81.7452\n",
      "Epoch 00326: val_loss did not improve from 7286.27979\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 5356.3564 - rmse: 70.5289 - val_loss: 7345.4551 - val_rmse: 92.5702\n",
      "Epoch 327/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3891.4985 - rmse: 62.3819\n",
      "Epoch 00327: val_loss did not improve from 7286.27979\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 5330.9985 - rmse: 71.7119 - val_loss: 7400.1206 - val_rmse: 92.0176\n",
      "Epoch 328/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6981.9077 - rmse: 83.5578\n",
      "Epoch 00328: val_loss did not improve from 7286.27979\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5402.2915 - rmse: 72.8920 - val_loss: 7489.2261 - val_rmse: 93.3940\n",
      "Epoch 329/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6365.9419 - rmse: 79.7868\n",
      "Epoch 00329: val_loss did not improve from 7286.27979\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 5513.0708 - rmse: 72.6844 - val_loss: 7314.9277 - val_rmse: 91.9017\n",
      "Epoch 330/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6453.8418 - rmse: 80.3358\n",
      "Epoch 00330: val_loss did not improve from 7286.27979\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5369.1426 - rmse: 73.6258 - val_loss: 7347.0132 - val_rmse: 91.4490\n",
      "Epoch 331/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7966.7368 - rmse: 89.2566\n",
      "Epoch 00331: val_loss improved from 7286.27979 to 7261.84668, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 5390.6968 - rmse: 72.8507 - val_loss: 7261.8467 - val_rmse: 91.8325\n",
      "Epoch 332/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7029.9434 - rmse: 83.8448\n",
      "Epoch 00332: val_loss improved from 7261.84668 to 7194.70898, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 5297.4438 - rmse: 70.0049 - val_loss: 7194.7090 - val_rmse: 91.1758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 333/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7162.6748 - rmse: 84.6326\n",
      "Epoch 00333: val_loss did not improve from 7194.70898\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 5331.7100 - rmse: 72.2914 - val_loss: 7310.6240 - val_rmse: 91.2292\n",
      "Epoch 334/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2822.5278 - rmse: 53.1275\n",
      "Epoch 00334: val_loss improved from 7194.70898 to 7100.41650, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 5227.7314 - rmse: 71.9898 - val_loss: 7100.4165 - val_rmse: 90.5869\n",
      "Epoch 335/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6154.1445 - rmse: 78.4484\n",
      "Epoch 00335: val_loss improved from 7100.41650 to 7064.46924, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 5141.7476 - rmse: 71.1592 - val_loss: 7064.4692 - val_rmse: 89.9841\n",
      "Epoch 336/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4517.1533 - rmse: 67.2098\n",
      "Epoch 00336: val_loss improved from 7064.46924 to 7030.90234, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 5173.6514 - rmse: 69.4399 - val_loss: 7030.9023 - val_rmse: 89.8534\n",
      "Epoch 337/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2334.2957 - rmse: 48.3145\n",
      "Epoch 00337: val_loss did not improve from 7030.90234\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 5137.2007 - rmse: 70.9872 - val_loss: 7143.1934 - val_rmse: 91.3098\n",
      "Epoch 338/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7434.0352 - rmse: 86.2208\n",
      "Epoch 00338: val_loss did not improve from 7030.90234\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 5159.0327 - rmse: 68.6519 - val_loss: 7152.5186 - val_rmse: 90.5548\n",
      "Epoch 339/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3920.0186 - rmse: 62.6101\n",
      "Epoch 00339: val_loss improved from 7030.90234 to 6993.19336, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 5099.0654 - rmse: 71.3232 - val_loss: 6993.1934 - val_rmse: 89.8701\n",
      "Epoch 340/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5509.2109 - rmse: 74.2241\n",
      "Epoch 00340: val_loss improved from 6993.19336 to 6987.95703, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5062.2510 - rmse: 71.5833 - val_loss: 6987.9570 - val_rmse: 89.7062\n",
      "Epoch 341/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4362.6968 - rmse: 66.0507\n",
      "Epoch 00341: val_loss improved from 6987.95703 to 6968.32324, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 5022.5166 - rmse: 70.4026 - val_loss: 6968.3232 - val_rmse: 89.5198\n",
      "Epoch 342/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6366.4580 - rmse: 79.7901\n",
      "Epoch 00342: val_loss did not improve from 6968.32324\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 5005.1348 - rmse: 68.1591 - val_loss: 6977.9580 - val_rmse: 89.7196\n",
      "Epoch 343/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2208.1992 - rmse: 46.9915\n",
      "Epoch 00343: val_loss improved from 6968.32324 to 6906.89404, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4984.3403 - rmse: 68.3490 - val_loss: 6906.8940 - val_rmse: 89.0670\n",
      "Epoch 344/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2193.4727 - rmse: 46.8345\n",
      "Epoch 00344: val_loss improved from 6906.89404 to 6843.56787, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4958.9810 - rmse: 68.9032 - val_loss: 6843.5679 - val_rmse: 88.7512\n",
      "Epoch 345/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3198.5544 - rmse: 56.5558\n",
      "Epoch 00345: val_loss did not improve from 6843.56787\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4948.2070 - rmse: 67.9229 - val_loss: 6846.5942 - val_rmse: 88.7741\n",
      "Epoch 346/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5790.0283 - rmse: 76.0922\n",
      "Epoch 00346: val_loss did not improve from 6843.56787\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4940.5752 - rmse: 69.5326 - val_loss: 6910.3022 - val_rmse: 88.7049\n",
      "Epoch 347/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7569.7671 - rmse: 87.0044\n",
      "Epoch 00347: val_loss did not improve from 6843.56787\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4893.8364 - rmse: 67.8952 - val_loss: 6862.5391 - val_rmse: 88.8069\n",
      "Epoch 348/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6000.1743 - rmse: 77.4608\n",
      "Epoch 00348: val_loss improved from 6843.56787 to 6770.84863, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4887.7832 - rmse: 69.7381 - val_loss: 6770.8486 - val_rmse: 88.0854\n",
      "Epoch 349/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3074.6404 - rmse: 55.4494\n",
      "Epoch 00349: val_loss improved from 6770.84863 to 6734.79785, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4872.8999 - rmse: 67.3447 - val_loss: 6734.7979 - val_rmse: 87.5932\n",
      "Epoch 350/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7285.2344 - rmse: 85.3536\n",
      "Epoch 00350: val_loss improved from 6734.79785 to 6672.33350, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 4850.9966 - rmse: 67.3492 - val_loss: 6672.3335 - val_rmse: 87.5695\n",
      "Epoch 351/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7290.9463 - rmse: 85.3870\n",
      "Epoch 00351: val_loss did not improve from 6672.33350\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4822.4302 - rmse: 68.0271 - val_loss: 6725.4116 - val_rmse: 87.7698\n",
      "Epoch 352/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3433.5198 - rmse: 58.5962\n",
      "Epoch 00352: val_loss did not improve from 6672.33350\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4813.9502 - rmse: 66.6678 - val_loss: 6761.5776 - val_rmse: 87.5953\n",
      "Epoch 353/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3917.7744 - rmse: 62.5921\n",
      "Epoch 00353: val_loss did not improve from 6672.33350\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4805.0713 - rmse: 68.1250 - val_loss: 6685.1831 - val_rmse: 86.9767\n",
      "Epoch 354/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3374.9946 - rmse: 58.0947\n",
      "Epoch 00354: val_loss improved from 6672.33350 to 6641.55762, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4808.8120 - rmse: 67.1640 - val_loss: 6641.5576 - val_rmse: 87.3806\n",
      "Epoch 355/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7453.9551 - rmse: 86.3363\n",
      "Epoch 00355: val_loss did not improve from 6641.55762\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4782.7427 - rmse: 65.8278 - val_loss: 6703.3882 - val_rmse: 87.7199\n",
      "Epoch 356/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2618.6758 - rmse: 51.1730\n",
      "Epoch 00356: val_loss did not improve from 6641.55762\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4761.3867 - rmse: 66.8399 - val_loss: 6703.3560 - val_rmse: 87.0186\n",
      "Epoch 357/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5938.4814 - rmse: 77.0615\n",
      "Epoch 00357: val_loss did not improve from 6641.55762\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4720.8198 - rmse: 69.0195 - val_loss: 6648.2393 - val_rmse: 86.8023\n",
      "Epoch 358/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4290.5181 - rmse: 65.5020\n",
      "Epoch 00358: val_loss improved from 6641.55762 to 6589.14307, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4708.2119 - rmse: 67.0080 - val_loss: 6589.1431 - val_rmse: 86.4557\n",
      "Epoch 359/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5002.5967 - rmse: 70.7290\n",
      "Epoch 00359: val_loss improved from 6589.14307 to 6551.54346, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4671.6558 - rmse: 66.3055 - val_loss: 6551.5435 - val_rmse: 86.5263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 360/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4234.7119 - rmse: 65.0747\n",
      "Epoch 00360: val_loss did not improve from 6551.54346\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 4668.1108 - rmse: 65.5925 - val_loss: 6599.9248 - val_rmse: 86.3812\n",
      "Epoch 361/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3510.2854 - rmse: 59.2477\n",
      "Epoch 00361: val_loss did not improve from 6551.54346\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4637.0352 - rmse: 66.0081 - val_loss: 6600.4546 - val_rmse: 86.4667\n",
      "Epoch 362/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3205.9915 - rmse: 56.6215\n",
      "Epoch 00362: val_loss did not improve from 6551.54346\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4620.3379 - rmse: 66.7431 - val_loss: 6566.7139 - val_rmse: 86.0725\n",
      "Epoch 363/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3970.0728 - rmse: 63.0085\n",
      "Epoch 00363: val_loss improved from 6551.54346 to 6457.00732, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 4613.0879 - rmse: 67.1986 - val_loss: 6457.0073 - val_rmse: 85.4335\n",
      "Epoch 364/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6531.7402 - rmse: 80.8192\n",
      "Epoch 00364: val_loss did not improve from 6457.00732\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4620.0215 - rmse: 67.0910 - val_loss: 6487.1182 - val_rmse: 86.1152\n",
      "Epoch 365/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 8884.5166 - rmse: 94.2577\n",
      "Epoch 00365: val_loss did not improve from 6457.00732\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4636.5073 - rmse: 65.5901 - val_loss: 6535.2402 - val_rmse: 86.1545\n",
      "Epoch 366/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6479.2109 - rmse: 80.4935\n",
      "Epoch 00366: val_loss did not improve from 6457.00732\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 4569.6069 - rmse: 65.2624 - val_loss: 6585.1758 - val_rmse: 85.8939\n",
      "Epoch 367/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3516.1846 - rmse: 59.2974\n",
      "Epoch 00367: val_loss did not improve from 6457.00732\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4549.5571 - rmse: 67.8738 - val_loss: 6484.3345 - val_rmse: 85.3051\n",
      "Epoch 368/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3695.4424 - rmse: 60.7901\n",
      "Epoch 00368: val_loss did not improve from 6457.00732\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4531.2051 - rmse: 66.4376 - val_loss: 6461.2217 - val_rmse: 85.6703\n",
      "Epoch 369/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5079.7725 - rmse: 71.2725\n",
      "Epoch 00369: val_loss did not improve from 6457.00732\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4519.2856 - rmse: 66.3627 - val_loss: 6547.0718 - val_rmse: 85.6835\n",
      "Epoch 370/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4255.0356 - rmse: 65.2306\n",
      "Epoch 00370: val_loss improved from 6457.00732 to 6412.86279, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4543.1982 - rmse: 66.5301 - val_loss: 6412.8628 - val_rmse: 85.0437\n",
      "Epoch 371/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4407.9180 - rmse: 66.3922\n",
      "Epoch 00371: val_loss improved from 6412.86279 to 6375.97168, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4495.9448 - rmse: 65.8229 - val_loss: 6375.9717 - val_rmse: 84.8315\n",
      "Epoch 372/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2910.1326 - rmse: 53.9456\n",
      "Epoch 00372: val_loss did not improve from 6375.97168\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 4460.9116 - rmse: 66.8812 - val_loss: 6393.7437 - val_rmse: 84.3049\n",
      "Epoch 373/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2856.0281 - rmse: 53.4418\n",
      "Epoch 00373: val_loss improved from 6375.97168 to 6295.49414, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4475.6548 - rmse: 63.5163 - val_loss: 6295.4941 - val_rmse: 84.6195\n",
      "Epoch 374/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4299.0776 - rmse: 65.5674\n",
      "Epoch 00374: val_loss did not improve from 6295.49414\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4476.3159 - rmse: 62.3704 - val_loss: 6376.5791 - val_rmse: 84.3438\n",
      "Epoch 375/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2776.0151 - rmse: 52.6879\n",
      "Epoch 00375: val_loss improved from 6295.49414 to 6287.16016, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4465.7671 - rmse: 66.7956 - val_loss: 6287.1602 - val_rmse: 83.8590\n",
      "Epoch 376/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5006.5693 - rmse: 70.7571\n",
      "Epoch 00376: val_loss did not improve from 6287.16016\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4441.3560 - rmse: 66.6685 - val_loss: 6394.5356 - val_rmse: 85.2840\n",
      "Epoch 377/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4820.1782 - rmse: 69.4275\n",
      "Epoch 00377: val_loss did not improve from 6287.16016\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4403.2148 - rmse: 62.7768 - val_loss: 6380.4688 - val_rmse: 84.4340\n",
      "Epoch 378/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4335.9653 - rmse: 65.8480\n",
      "Epoch 00378: val_loss did not improve from 6287.16016\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4359.1255 - rmse: 63.9703 - val_loss: 6324.9385 - val_rmse: 84.7543\n",
      "Epoch 379/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5867.4189 - rmse: 76.5991\n",
      "Epoch 00379: val_loss improved from 6287.16016 to 6276.50342, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4328.9761 - rmse: 65.9824 - val_loss: 6276.5034 - val_rmse: 83.8047\n",
      "Epoch 380/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4545.6348 - rmse: 67.4213\n",
      "Epoch 00380: val_loss improved from 6276.50342 to 6234.05420, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 4326.8057 - rmse: 64.3074 - val_loss: 6234.0542 - val_rmse: 83.5333\n",
      "Epoch 381/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4044.0591 - rmse: 63.5929\n",
      "Epoch 00381: val_loss improved from 6234.05420 to 6215.39014, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 4293.0674 - rmse: 65.9107 - val_loss: 6215.3901 - val_rmse: 83.5577\n",
      "Epoch 382/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2879.0537 - rmse: 53.6568\n",
      "Epoch 00382: val_loss improved from 6215.39014 to 6181.91406, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4281.8623 - rmse: 64.3391 - val_loss: 6181.9141 - val_rmse: 83.1784\n",
      "Epoch 383/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3882.9768 - rmse: 62.3135\n",
      "Epoch 00383: val_loss improved from 6181.91406 to 6091.12061, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4293.6890 - rmse: 64.5077 - val_loss: 6091.1206 - val_rmse: 83.0112\n",
      "Epoch 384/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5800.1196 - rmse: 76.1585\n",
      "Epoch 00384: val_loss did not improve from 6091.12061\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4254.8501 - rmse: 63.9456 - val_loss: 6136.6235 - val_rmse: 83.2270\n",
      "Epoch 385/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3421.9985 - rmse: 58.4979\n",
      "Epoch 00385: val_loss did not improve from 6091.12061\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4236.7139 - rmse: 65.2297 - val_loss: 6174.5088 - val_rmse: 82.8452\n",
      "Epoch 386/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4168.7051 - rmse: 64.5655\n",
      "Epoch 00386: val_loss did not improve from 6091.12061\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 4255.0312 - rmse: 63.8302 - val_loss: 6149.2852 - val_rmse: 82.7435\n",
      "Epoch 387/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 7242.4434 - rmse: 85.1025\n",
      "Epoch 00387: val_loss did not improve from 6091.12061\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4206.1133 - rmse: 62.7857 - val_loss: 6126.9512 - val_rmse: 83.0372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 388/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3831.7114 - rmse: 61.9008\n",
      "Epoch 00388: val_loss improved from 6091.12061 to 6083.90137, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 4186.1714 - rmse: 64.5795 - val_loss: 6083.9014 - val_rmse: 82.5599\n",
      "Epoch 389/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3882.0188 - rmse: 62.3058\n",
      "Epoch 00389: val_loss improved from 6083.90137 to 6083.31152, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 4164.9170 - rmse: 63.3997 - val_loss: 6083.3115 - val_rmse: 82.6603\n",
      "Epoch 390/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2483.9666 - rmse: 49.8394\n",
      "Epoch 00390: val_loss did not improve from 6083.31152\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4185.6235 - rmse: 64.7255 - val_loss: 6101.4609 - val_rmse: 82.7023\n",
      "Epoch 391/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3127.5408 - rmse: 55.9244\n",
      "Epoch 00391: val_loss improved from 6083.31152 to 6050.94141, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4155.9443 - rmse: 64.7713 - val_loss: 6050.9414 - val_rmse: 82.0888\n",
      "Epoch 392/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2019.9558 - rmse: 44.9439\n",
      "Epoch 00392: val_loss did not improve from 6050.94141\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4128.2759 - rmse: 61.7785 - val_loss: 6120.0151 - val_rmse: 82.9127\n",
      "Epoch 393/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5647.4443 - rmse: 75.1495\n",
      "Epoch 00393: val_loss did not improve from 6050.94141\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4196.7070 - rmse: 62.3851 - val_loss: 6131.8257 - val_rmse: 82.4435\n",
      "Epoch 394/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2397.7363 - rmse: 48.9667\n",
      "Epoch 00394: val_loss did not improve from 6050.94141\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4099.3574 - rmse: 62.1836 - val_loss: 6078.0610 - val_rmse: 82.1093\n",
      "Epoch 395/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3342.2490 - rmse: 57.8122\n",
      "Epoch 00395: val_loss improved from 6050.94141 to 5997.91602, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4114.3242 - rmse: 64.6327 - val_loss: 5997.9160 - val_rmse: 81.7586\n",
      "Epoch 396/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6339.8940 - rmse: 79.6235\n",
      "Epoch 00396: val_loss improved from 5997.91602 to 5959.52686, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4110.8452 - rmse: 60.8792 - val_loss: 5959.5269 - val_rmse: 81.5993\n",
      "Epoch 397/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3536.0449 - rmse: 59.4647\n",
      "Epoch 00397: val_loss did not improve from 5959.52686\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4059.9297 - rmse: 60.1985 - val_loss: 6001.5566 - val_rmse: 81.5470\n",
      "Epoch 398/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2045.9641 - rmse: 45.2323\n",
      "Epoch 00398: val_loss did not improve from 5959.52686\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 4064.8301 - rmse: 63.6832 - val_loss: 5964.5298 - val_rmse: 81.4421\n",
      "Epoch 399/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3311.6775 - rmse: 57.5472\n",
      "Epoch 00399: val_loss did not improve from 5959.52686\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4091.6492 - rmse: 62.5797 - val_loss: 6126.3271 - val_rmse: 82.4723\n",
      "Epoch 400/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3643.5017 - rmse: 60.3614\n",
      "Epoch 00400: val_loss did not improve from 5959.52686\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4077.2422 - rmse: 64.1977 - val_loss: 5990.6724 - val_rmse: 81.3789\n",
      "Epoch 401/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5245.5928 - rmse: 72.4265\n",
      "Epoch 00401: val_loss did not improve from 5959.52686\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4018.5071 - rmse: 63.3944 - val_loss: 5987.4448 - val_rmse: 81.2777\n",
      "Epoch 402/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3205.1870 - rmse: 56.6144\n",
      "Epoch 00402: val_loss did not improve from 5959.52686\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4030.8245 - rmse: 63.6260 - val_loss: 5981.3267 - val_rmse: 81.8734\n",
      "Epoch 403/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6011.3936 - rmse: 77.5332\n",
      "Epoch 00403: val_loss improved from 5959.52686 to 5902.86475, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4020.8503 - rmse: 61.7110 - val_loss: 5902.8647 - val_rmse: 80.9360\n",
      "Epoch 404/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4625.4058 - rmse: 68.0103\n",
      "Epoch 00404: val_loss did not improve from 5902.86475\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4017.6121 - rmse: 61.5942 - val_loss: 5962.8784 - val_rmse: 81.0221\n",
      "Epoch 405/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4854.2432 - rmse: 69.6724\n",
      "Epoch 00405: val_loss did not improve from 5902.86475\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3970.4192 - rmse: 62.6559 - val_loss: 5950.6226 - val_rmse: 81.5792\n",
      "Epoch 406/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4815.1689 - rmse: 69.3914\n",
      "Epoch 00406: val_loss improved from 5902.86475 to 5794.90576, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 4022.7483 - rmse: 62.8325 - val_loss: 5794.9058 - val_rmse: 80.4350\n",
      "Epoch 407/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5208.4878 - rmse: 72.1699\n",
      "Epoch 00407: val_loss did not improve from 5794.90576\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3938.2222 - rmse: 61.3066 - val_loss: 5797.8906 - val_rmse: 80.1273\n",
      "Epoch 408/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3346.0796 - rmse: 57.8453\n",
      "Epoch 00408: val_loss improved from 5794.90576 to 5794.36084, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3906.1057 - rmse: 63.1622 - val_loss: 5794.3608 - val_rmse: 80.2917\n",
      "Epoch 409/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2860.0293 - rmse: 53.4792\n",
      "Epoch 00409: val_loss did not improve from 5794.36084\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3873.7341 - rmse: 61.9512 - val_loss: 5867.9067 - val_rmse: 80.1321\n",
      "Epoch 410/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2697.3828 - rmse: 51.9363\n",
      "Epoch 00410: val_loss did not improve from 5794.36084\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3942.0076 - rmse: 61.1119 - val_loss: 5801.3799 - val_rmse: 79.9813\n",
      "Epoch 411/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2856.8181 - rmse: 53.4492\n",
      "Epoch 00411: val_loss improved from 5794.36084 to 5759.06250, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3927.1670 - rmse: 62.4484 - val_loss: 5759.0625 - val_rmse: 80.0750\n",
      "Epoch 412/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2999.4053 - rmse: 54.7668\n",
      "Epoch 00412: val_loss improved from 5759.06250 to 5751.26172, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3854.1355 - rmse: 62.3048 - val_loss: 5751.2617 - val_rmse: 79.8920\n",
      "Epoch 413/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2682.8916 - rmse: 51.7966\n",
      "Epoch 00413: val_loss did not improve from 5751.26172\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3863.0725 - rmse: 60.7771 - val_loss: 5751.4883 - val_rmse: 79.9992\n",
      "Epoch 414/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3536.4727 - rmse: 59.4683\n",
      "Epoch 00414: val_loss improved from 5751.26172 to 5741.49414, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 3828.6326 - rmse: 61.1076 - val_loss: 5741.4941 - val_rmse: 79.6783\n",
      "Epoch 415/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4195.3994 - rmse: 64.7719\n",
      "Epoch 00415: val_loss improved from 5741.49414 to 5731.51416, saving model to weights_rossmann.best.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 20ms/step - loss: 3795.7988 - rmse: 60.6081 - val_loss: 5731.5142 - val_rmse: 79.5850\n",
      "Epoch 416/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3244.6777 - rmse: 56.9621\n",
      "Epoch 00416: val_loss improved from 5731.51416 to 5632.39746, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 24ms/step - loss: 3795.1257 - rmse: 61.9389 - val_loss: 5632.3975 - val_rmse: 78.6491\n",
      "Epoch 417/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3296.2832 - rmse: 57.4133\n",
      "Epoch 00417: val_loss improved from 5632.39746 to 5563.78027, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3782.3354 - rmse: 59.4869 - val_loss: 5563.7803 - val_rmse: 78.5796\n",
      "Epoch 418/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3936.6475 - rmse: 62.7427\n",
      "Epoch 00418: val_loss did not improve from 5563.78027\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3781.6868 - rmse: 60.0291 - val_loss: 5638.1343 - val_rmse: 78.7036\n",
      "Epoch 419/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4187.3057 - rmse: 64.7094\n",
      "Epoch 00419: val_loss did not improve from 5563.78027\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3788.2566 - rmse: 61.5487 - val_loss: 5667.7959 - val_rmse: 79.4823\n",
      "Epoch 420/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3609.6328 - rmse: 60.0802\n",
      "Epoch 00420: val_loss did not improve from 5563.78027\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3771.7058 - rmse: 61.8795 - val_loss: 5628.3086 - val_rmse: 78.6540\n",
      "Epoch 421/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2371.5938 - rmse: 48.6990\n",
      "Epoch 00421: val_loss did not improve from 5563.78027\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3731.5400 - rmse: 61.3696 - val_loss: 5650.1001 - val_rmse: 78.7724\n",
      "Epoch 422/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3092.0181 - rmse: 55.6059\n",
      "Epoch 00422: val_loss did not improve from 5563.78027\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3721.8491 - rmse: 60.8025 - val_loss: 5654.2573 - val_rmse: 79.2269\n",
      "Epoch 423/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4547.8569 - rmse: 67.4378\n",
      "Epoch 00423: val_loss did not improve from 5563.78027\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3749.3533 - rmse: 60.4407 - val_loss: 5607.1084 - val_rmse: 78.5099\n",
      "Epoch 424/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3307.1877 - rmse: 57.5082\n",
      "Epoch 00424: val_loss improved from 5563.78027 to 5558.00977, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3710.8049 - rmse: 61.1904 - val_loss: 5558.0098 - val_rmse: 78.1674\n",
      "Epoch 425/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3137.5662 - rmse: 56.0140\n",
      "Epoch 00425: val_loss did not improve from 5558.00977\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3714.0063 - rmse: 59.5431 - val_loss: 5683.8677 - val_rmse: 79.5245\n",
      "Epoch 426/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1643.0527 - rmse: 40.5346\n",
      "Epoch 00426: val_loss did not improve from 5558.00977\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3729.2695 - rmse: 58.0009 - val_loss: 5618.5171 - val_rmse: 78.4103\n",
      "Epoch 427/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2328.1301 - rmse: 48.2507\n",
      "Epoch 00427: val_loss did not improve from 5558.00977\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3706.1499 - rmse: 59.1229 - val_loss: 5582.4941 - val_rmse: 77.9872\n",
      "Epoch 428/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4823.4131 - rmse: 69.4508\n",
      "Epoch 00428: val_loss improved from 5558.00977 to 5535.08398, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3718.7163 - rmse: 60.1103 - val_loss: 5535.0840 - val_rmse: 78.2347\n",
      "Epoch 429/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4774.8628 - rmse: 69.1004\n",
      "Epoch 00429: val_loss improved from 5535.08398 to 5531.96777, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3647.0991 - rmse: 60.3139 - val_loss: 5531.9678 - val_rmse: 77.7263\n",
      "Epoch 430/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4956.4648 - rmse: 70.4022\n",
      "Epoch 00430: val_loss did not improve from 5531.96777\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3624.6067 - rmse: 60.2881 - val_loss: 5580.6758 - val_rmse: 78.5281\n",
      "Epoch 431/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6889.0186 - rmse: 83.0001\n",
      "Epoch 00431: val_loss did not improve from 5531.96777\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3698.3857 - rmse: 58.8704 - val_loss: 5606.4033 - val_rmse: 78.6911\n",
      "Epoch 432/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2640.8196 - rmse: 51.3889\n",
      "Epoch 00432: val_loss did not improve from 5531.96777\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3660.3232 - rmse: 60.9078 - val_loss: 5559.1982 - val_rmse: 77.3586\n",
      "Epoch 433/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2158.8066 - rmse: 46.4630\n",
      "Epoch 00433: val_loss improved from 5531.96777 to 5460.18799, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3677.1650 - rmse: 59.1502 - val_loss: 5460.1880 - val_rmse: 76.9990\n",
      "Epoch 434/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2858.9438 - rmse: 53.4691\n",
      "Epoch 00434: val_loss did not improve from 5460.18799\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3632.6326 - rmse: 58.5610 - val_loss: 5682.2729 - val_rmse: 79.0453\n",
      "Epoch 435/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2799.7996 - rmse: 52.9131\n",
      "Epoch 00435: val_loss did not improve from 5460.18799\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3649.3625 - rmse: 60.1215 - val_loss: 5571.7095 - val_rmse: 77.5443\n",
      "Epoch 436/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1843.6331 - rmse: 42.9375\n",
      "Epoch 00436: val_loss did not improve from 5460.18799\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3558.4663 - rmse: 57.5647 - val_loss: 5490.0615 - val_rmse: 77.5105\n",
      "Epoch 437/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3913.9214 - rmse: 62.5613\n",
      "Epoch 00437: val_loss improved from 5460.18799 to 5435.33203, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3560.3997 - rmse: 57.2745 - val_loss: 5435.3320 - val_rmse: 76.9379\n",
      "Epoch 438/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3296.4905 - rmse: 57.4151\n",
      "Epoch 00438: val_loss improved from 5435.33203 to 5432.44189, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3565.9512 - rmse: 55.9000 - val_loss: 5432.4419 - val_rmse: 76.5921\n",
      "Epoch 439/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1974.6285 - rmse: 44.4368\n",
      "Epoch 00439: val_loss improved from 5432.44189 to 5379.79492, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3572.1650 - rmse: 59.5580 - val_loss: 5379.7949 - val_rmse: 76.2334\n",
      "Epoch 440/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4372.4077 - rmse: 66.1242\n",
      "Epoch 00440: val_loss did not improve from 5379.79492\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3539.3813 - rmse: 59.0343 - val_loss: 5491.2676 - val_rmse: 77.4389\n",
      "Epoch 441/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3198.4126 - rmse: 56.5545\n",
      "Epoch 00441: val_loss did not improve from 5379.79492\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3512.4824 - rmse: 59.7345 - val_loss: 5490.3384 - val_rmse: 77.1215\n",
      "Epoch 442/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1392.1174 - rmse: 37.3111\n",
      "Epoch 00442: val_loss did not improve from 5379.79492\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3486.4766 - rmse: 57.8050 - val_loss: 5443.7686 - val_rmse: 76.8899\n",
      "Epoch 443/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2430.1558 - rmse: 49.2966\n",
      "Epoch 00443: val_loss did not improve from 5379.79492\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3471.9373 - rmse: 57.7458 - val_loss: 5389.2124 - val_rmse: 76.6971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 444/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2912.8218 - rmse: 53.9706\n",
      "Epoch 00444: val_loss did not improve from 5379.79492\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3468.0881 - rmse: 57.2525 - val_loss: 5387.8037 - val_rmse: 76.5073\n",
      "Epoch 445/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2094.8877 - rmse: 45.7699\n",
      "Epoch 00445: val_loss improved from 5379.79492 to 5309.84668, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3457.1245 - rmse: 58.0119 - val_loss: 5309.8467 - val_rmse: 75.7291\n",
      "Epoch 446/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4371.2002 - rmse: 66.1151\n",
      "Epoch 00446: val_loss improved from 5309.84668 to 5295.50488, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3439.1597 - rmse: 58.6398 - val_loss: 5295.5049 - val_rmse: 75.7759\n",
      "Epoch 447/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5025.1016 - rmse: 70.8879\n",
      "Epoch 00447: val_loss did not improve from 5295.50488\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3441.7383 - rmse: 57.4560 - val_loss: 5384.4023 - val_rmse: 76.7328\n",
      "Epoch 448/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2079.6475 - rmse: 45.6032\n",
      "Epoch 00448: val_loss did not improve from 5295.50488\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3422.5869 - rmse: 57.0516 - val_loss: 5390.8052 - val_rmse: 76.4104\n",
      "Epoch 449/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3726.0430 - rmse: 61.0413\n",
      "Epoch 00449: val_loss did not improve from 5295.50488\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3408.8049 - rmse: 56.0086 - val_loss: 5328.6968 - val_rmse: 75.9028\n",
      "Epoch 450/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2801.1768 - rmse: 52.9261\n",
      "Epoch 00450: val_loss improved from 5295.50488 to 5276.67578, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3423.2966 - rmse: 58.2236 - val_loss: 5276.6758 - val_rmse: 75.8778\n",
      "Epoch 451/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4262.9399 - rmse: 65.2912\n",
      "Epoch 00451: val_loss did not improve from 5276.67578\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3390.3682 - rmse: 55.7509 - val_loss: 5286.1240 - val_rmse: 75.3077\n",
      "Epoch 452/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2060.4414 - rmse: 45.3921\n",
      "Epoch 00452: val_loss did not improve from 5276.67578\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3378.1509 - rmse: 56.9681 - val_loss: 5280.8354 - val_rmse: 75.7728\n",
      "Epoch 453/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5155.8457 - rmse: 71.8042\n",
      "Epoch 00453: val_loss improved from 5276.67578 to 5207.71094, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3381.7266 - rmse: 56.1650 - val_loss: 5207.7109 - val_rmse: 75.0908\n",
      "Epoch 454/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2652.0696 - rmse: 51.4982\n",
      "Epoch 00454: val_loss improved from 5207.71094 to 5180.08008, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3413.1790 - rmse: 58.5304 - val_loss: 5180.0801 - val_rmse: 74.8221\n",
      "Epoch 455/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5124.6846 - rmse: 71.5869\n",
      "Epoch 00455: val_loss did not improve from 5180.08008\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3335.7654 - rmse: 55.9155 - val_loss: 5184.7959 - val_rmse: 75.1268\n",
      "Epoch 456/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3279.3691 - rmse: 57.2658\n",
      "Epoch 00456: val_loss improved from 5180.08008 to 5167.99854, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3348.4080 - rmse: 55.7028 - val_loss: 5167.9985 - val_rmse: 74.6221\n",
      "Epoch 457/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1773.0188 - rmse: 42.1072\n",
      "Epoch 00457: val_loss did not improve from 5167.99854\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3326.6921 - rmse: 56.0685 - val_loss: 5215.0898 - val_rmse: 75.6040\n",
      "Epoch 458/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3144.8848 - rmse: 56.0793\n",
      "Epoch 00458: val_loss did not improve from 5167.99854\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3367.6978 - rmse: 58.4326 - val_loss: 5172.8389 - val_rmse: 74.7064\n",
      "Epoch 459/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3825.4243 - rmse: 61.8500\n",
      "Epoch 00459: val_loss did not improve from 5167.99854\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3309.5288 - rmse: 56.9046 - val_loss: 5190.2881 - val_rmse: 75.1233\n",
      "Epoch 460/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3994.4526 - rmse: 63.2017\n",
      "Epoch 00460: val_loss improved from 5167.99854 to 5139.36914, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3297.5942 - rmse: 56.4032 - val_loss: 5139.3691 - val_rmse: 74.5234\n",
      "Epoch 461/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1867.1543 - rmse: 43.2106\n",
      "Epoch 00461: val_loss improved from 5139.36914 to 5085.89404, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3288.5164 - rmse: 57.0648 - val_loss: 5085.8940 - val_rmse: 74.1380\n",
      "Epoch 462/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4509.5601 - rmse: 67.1533\n",
      "Epoch 00462: val_loss did not improve from 5085.89404\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3275.0500 - rmse: 54.9095 - val_loss: 5139.1416 - val_rmse: 74.6585\n",
      "Epoch 463/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2354.0796 - rmse: 48.5189\n",
      "Epoch 00463: val_loss did not improve from 5085.89404\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3273.4458 - rmse: 55.7131 - val_loss: 5124.6323 - val_rmse: 74.4940\n",
      "Epoch 464/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2726.3091 - rmse: 52.2141\n",
      "Epoch 00464: val_loss did not improve from 5085.89404\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3259.2979 - rmse: 55.9493 - val_loss: 5148.6909 - val_rmse: 74.0494\n",
      "Epoch 465/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3213.0303 - rmse: 56.6836\n",
      "Epoch 00465: val_loss did not improve from 5085.89404\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3281.5742 - rmse: 56.9851 - val_loss: 5127.3447 - val_rmse: 74.2552\n",
      "Epoch 466/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2852.2300 - rmse: 53.4063\n",
      "Epoch 00466: val_loss did not improve from 5085.89404\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3256.5500 - rmse: 54.2363 - val_loss: 5142.0283 - val_rmse: 74.6577\n",
      "Epoch 467/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4023.4041 - rmse: 63.4303\n",
      "Epoch 00467: val_loss did not improve from 5085.89404\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3241.7192 - rmse: 54.8220 - val_loss: 5139.5073 - val_rmse: 74.2136\n",
      "Epoch 468/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3906.2822 - rmse: 62.5003\n",
      "Epoch 00468: val_loss did not improve from 5085.89404\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3219.6060 - rmse: 56.2940 - val_loss: 5092.5776 - val_rmse: 73.8182\n",
      "Epoch 469/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1854.1877 - rmse: 43.0603\n",
      "Epoch 00469: val_loss improved from 5085.89404 to 5078.39062, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3217.9026 - rmse: 56.9173 - val_loss: 5078.3906 - val_rmse: 73.7680\n",
      "Epoch 470/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2393.6235 - rmse: 48.9247\n",
      "Epoch 00470: val_loss did not improve from 5078.39062\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3208.9934 - rmse: 55.8242 - val_loss: 5089.6343 - val_rmse: 73.6651\n",
      "Epoch 471/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2074.5330 - rmse: 45.5470\n",
      "Epoch 00471: val_loss did not improve from 5078.39062\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3187.4368 - rmse: 54.9003 - val_loss: 5086.2075 - val_rmse: 73.8242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 472/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4653.6182 - rmse: 68.2174\n",
      "Epoch 00472: val_loss did not improve from 5078.39062\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3192.0054 - rmse: 54.7771 - val_loss: 5138.5986 - val_rmse: 74.3356\n",
      "Epoch 473/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5241.1396 - rmse: 72.3957\n",
      "Epoch 00473: val_loss did not improve from 5078.39062\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3179.9365 - rmse: 55.4568 - val_loss: 5111.5327 - val_rmse: 73.7601\n",
      "Epoch 474/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1411.2394 - rmse: 37.5665\n",
      "Epoch 00474: val_loss improved from 5078.39062 to 5049.38086, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3168.7834 - rmse: 53.7690 - val_loss: 5049.3809 - val_rmse: 73.1563\n",
      "Epoch 475/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2846.8655 - rmse: 53.3560\n",
      "Epoch 00475: val_loss improved from 5049.38086 to 5019.44580, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3162.9299 - rmse: 55.5594 - val_loss: 5019.4458 - val_rmse: 73.3360\n",
      "Epoch 476/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1913.9750 - rmse: 43.7490\n",
      "Epoch 00476: val_loss did not improve from 5019.44580\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3186.0181 - rmse: 56.4410 - val_loss: 5045.4932 - val_rmse: 73.0302\n",
      "Epoch 477/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1473.6473 - rmse: 38.3881\n",
      "Epoch 00477: val_loss improved from 5019.44580 to 4967.44678, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3150.6243 - rmse: 56.0220 - val_loss: 4967.4468 - val_rmse: 72.5719\n",
      "Epoch 478/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 6693.9614 - rmse: 81.8166\n",
      "Epoch 00478: val_loss did not improve from 4967.44678\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3188.7014 - rmse: 53.2894 - val_loss: 5130.9644 - val_rmse: 74.2272\n",
      "Epoch 479/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3559.3579 - rmse: 59.6604\n",
      "Epoch 00479: val_loss did not improve from 4967.44678\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3184.0354 - rmse: 55.0012 - val_loss: 5154.1147 - val_rmse: 74.2617\n",
      "Epoch 480/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1781.7803 - rmse: 42.2111\n",
      "Epoch 00480: val_loss did not improve from 4967.44678\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3137.1716 - rmse: 54.2287 - val_loss: 5092.6865 - val_rmse: 73.2752\n",
      "Epoch 481/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2252.5698 - rmse: 47.4612\n",
      "Epoch 00481: val_loss improved from 4967.44678 to 4922.91162, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3142.7375 - rmse: 54.2102 - val_loss: 4922.9116 - val_rmse: 72.2925\n",
      "Epoch 482/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4790.5962 - rmse: 69.2141\n",
      "Epoch 00482: val_loss did not improve from 4922.91162\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3160.4238 - rmse: 55.1849 - val_loss: 4997.0645 - val_rmse: 73.1313\n",
      "Epoch 483/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1616.7041 - rmse: 40.2083\n",
      "Epoch 00483: val_loss did not improve from 4922.91162\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3133.5049 - rmse: 53.1576 - val_loss: 4943.1636 - val_rmse: 72.4224\n",
      "Epoch 484/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2094.9165 - rmse: 45.7703\n",
      "Epoch 00484: val_loss did not improve from 4922.91162\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3102.1160 - rmse: 54.3479 - val_loss: 5041.5874 - val_rmse: 72.5847\n",
      "Epoch 485/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2929.6355 - rmse: 54.1261\n",
      "Epoch 00485: val_loss did not improve from 4922.91162\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3155.3823 - rmse: 54.5280 - val_loss: 5123.9956 - val_rmse: 74.1784\n",
      "Epoch 486/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4473.5459 - rmse: 66.8846\n",
      "Epoch 00486: val_loss did not improve from 4922.91162\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3152.5803 - rmse: 55.7646 - val_loss: 4987.9927 - val_rmse: 72.7181\n",
      "Epoch 487/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3802.3467 - rmse: 61.6632\n",
      "Epoch 00487: val_loss did not improve from 4922.91162\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3116.4199 - rmse: 53.7994 - val_loss: 4939.9351 - val_rmse: 72.1588\n",
      "Epoch 488/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 5582.2002 - rmse: 74.7141\n",
      "Epoch 00488: val_loss did not improve from 4922.91162\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3072.4780 - rmse: 53.3456 - val_loss: 4984.6279 - val_rmse: 73.2959\n",
      "Epoch 489/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4410.6982 - rmse: 66.4131\n",
      "Epoch 00489: val_loss improved from 4922.91162 to 4823.31299, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3116.5879 - rmse: 55.3466 - val_loss: 4823.3130 - val_rmse: 71.3827\n",
      "Epoch 490/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3103.2251 - rmse: 55.7066\n",
      "Epoch 00490: val_loss improved from 4823.31299 to 4773.89355, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3137.3579 - rmse: 54.3175 - val_loss: 4773.8936 - val_rmse: 70.8280\n",
      "Epoch 491/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2153.6497 - rmse: 46.4074\n",
      "Epoch 00491: val_loss improved from 4773.89355 to 4755.38867, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3097.7600 - rmse: 53.7851 - val_loss: 4755.3887 - val_rmse: 71.1974\n",
      "Epoch 492/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2409.6958 - rmse: 49.0887\n",
      "Epoch 00492: val_loss improved from 4755.38867 to 4741.98145, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3050.6584 - rmse: 55.6948 - val_loss: 4741.9814 - val_rmse: 71.0068\n",
      "Epoch 493/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2759.0037 - rmse: 52.5262\n",
      "Epoch 00493: val_loss did not improve from 4741.98145\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3000.0164 - rmse: 52.9692 - val_loss: 4838.2441 - val_rmse: 71.6175\n",
      "Epoch 494/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2919.3721 - rmse: 54.0312\n",
      "Epoch 00494: val_loss did not improve from 4741.98145\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3003.8259 - rmse: 53.2967 - val_loss: 4874.5352 - val_rmse: 72.0484\n",
      "Epoch 495/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3403.7273 - rmse: 58.3415\n",
      "Epoch 00495: val_loss did not improve from 4741.98145\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3009.1062 - rmse: 52.5749 - val_loss: 4916.1519 - val_rmse: 72.4158\n",
      "Epoch 496/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4412.9136 - rmse: 66.4298\n",
      "Epoch 00496: val_loss did not improve from 4741.98145\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3006.4866 - rmse: 54.3688 - val_loss: 4951.5752 - val_rmse: 71.5816\n",
      "Epoch 497/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1481.9685 - rmse: 38.4963\n",
      "Epoch 00497: val_loss did not improve from 4741.98145\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3030.0542 - rmse: 53.1405 - val_loss: 4865.1377 - val_rmse: 71.9973\n",
      "Epoch 498/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2333.9099 - rmse: 48.3106\n",
      "Epoch 00498: val_loss improved from 4741.98145 to 4734.27441, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3066.8101 - rmse: 52.4163 - val_loss: 4734.2744 - val_rmse: 70.6862\n",
      "Epoch 499/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1952.3804 - rmse: 44.1857\n",
      "Epoch 00499: val_loss did not improve from 4734.27441\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2984.0178 - rmse: 52.4033 - val_loss: 4780.3979 - val_rmse: 70.6583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3353.2190 - rmse: 57.9070\n",
      "Epoch 00500: val_loss did not improve from 4734.27441\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2987.5647 - rmse: 52.8081 - val_loss: 4813.5068 - val_rmse: 71.5349\n",
      "Epoch 501/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3487.0879 - rmse: 59.0516\n",
      "Epoch 00501: val_loss did not improve from 4734.27441\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2938.8826 - rmse: 53.9601 - val_loss: 4766.6685 - val_rmse: 70.5979\n",
      "Epoch 502/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2439.3428 - rmse: 49.3897\n",
      "Epoch 00502: val_loss did not improve from 4734.27441\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2946.6882 - rmse: 53.1211 - val_loss: 4807.4248 - val_rmse: 71.4571\n",
      "Epoch 503/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2374.5920 - rmse: 48.7298\n",
      "Epoch 00503: val_loss did not improve from 4734.27441\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2919.1331 - rmse: 54.0468 - val_loss: 4771.4312 - val_rmse: 70.7884\n",
      "Epoch 504/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1894.7925 - rmse: 43.5292\n",
      "Epoch 00504: val_loss did not improve from 4734.27441\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2915.2695 - rmse: 53.3005 - val_loss: 4745.0371 - val_rmse: 70.8703\n",
      "Epoch 505/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1777.4834 - rmse: 42.1602\n",
      "Epoch 00505: val_loss improved from 4734.27441 to 4711.60107, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2922.5496 - rmse: 53.3415 - val_loss: 4711.6011 - val_rmse: 70.5478\n",
      "Epoch 506/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2291.0608 - rmse: 47.8650\n",
      "Epoch 00506: val_loss did not improve from 4711.60107\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2906.5828 - rmse: 52.8591 - val_loss: 4750.0132 - val_rmse: 70.4441\n",
      "Epoch 507/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2258.5369 - rmse: 47.5241\n",
      "Epoch 00507: val_loss improved from 4711.60107 to 4648.78076, saving model to weights_rossmann.best.hdf5\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2913.9775 - rmse: 53.2351 - val_loss: 4648.7808 - val_rmse: 69.7862\n",
      "Epoch 508/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2175.6914 - rmse: 46.6443\n",
      "Epoch 00508: val_loss did not improve from 4648.78076\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2880.4700 - rmse: 52.3865 - val_loss: 4730.2739 - val_rmse: 70.6822\n",
      "Epoch 509/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1908.6860 - rmse: 43.6885\n",
      "Epoch 00509: val_loss did not improve from 4648.78076\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2914.3022 - rmse: 53.5091 - val_loss: 4861.0044 - val_rmse: 70.7490\n",
      "Epoch 510/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3690.1353 - rmse: 60.7465\n",
      "Epoch 00510: val_loss did not improve from 4648.78076\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2960.4084 - rmse: 54.5917 - val_loss: 4693.6582 - val_rmse: 70.0984\n",
      "Epoch 511/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 4211.2266 - rmse: 64.8940\n",
      "Epoch 00511: val_loss did not improve from 4648.78076\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2921.4165 - rmse: 52.9905 - val_loss: 4804.5601 - val_rmse: 71.3975\n",
      "Epoch 512/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3843.7256 - rmse: 61.9978\n",
      "Epoch 00512: val_loss did not improve from 4648.78076\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2897.5310 - rmse: 52.3963 - val_loss: 4881.0127 - val_rmse: 71.0685\n",
      "Epoch 513/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3901.4819 - rmse: 62.4618\n",
      "Epoch 00513: val_loss did not improve from 4648.78076\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2925.1750 - rmse: 54.0696 - val_loss: 4750.4136 - val_rmse: 70.5696\n",
      "Epoch 514/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2615.8506 - rmse: 51.1454\n",
      "Epoch 00514: val_loss did not improve from 4648.78076\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2865.0432 - rmse: 54.0479 - val_loss: 4781.7075 - val_rmse: 70.6559\n",
      "Epoch 515/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 2194.6484 - rmse: 46.8471\n",
      "Epoch 00515: val_loss did not improve from 4648.78076\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2854.8289 - rmse: 52.7758 - val_loss: 4811.5894 - val_rmse: 70.6555\n",
      "Epoch 516/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 3719.9268 - rmse: 60.9912\n",
      "Epoch 00516: val_loss did not improve from 4648.78076\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2866.9624 - rmse: 51.9048 - val_loss: 4762.8296 - val_rmse: 70.7241\n",
      "Epoch 517/1000\n",
      "1/5 [=====>........................] - ETA: 0s - loss: 1486.3345 - rmse: 38.5530\n",
      "Epoch 00517: val_loss did not improve from 4648.78076\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2836.5159 - rmse: 51.1892 - val_loss: 4701.8169 - val_rmse: 70.3167\n",
      "Epoch 00517: early stopping\n"
     ]
    }
   ],
   "source": [
    "print('Fit model...')\n",
    "# Hdf5 is the best format to store the model \n",
    "filepath=\"weights_rossmann.best.hdf5\"\n",
    "# For saving the best weights \n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# Stopping the epochs if there is no change in loss, so that we do not overfit the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "log = model.fit(X_train, Y_train,\n",
    "          validation_split=0.20, batch_size=batch_size, epochs=nb_epoch, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For plotting the error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def show_info(model, X, y, log, weights = None):\n",
    "    '''\n",
    "    Show metrics about the evaluation model and plots about loss, rmse and rmspe\n",
    "    '''\n",
    "    if (log != None):\n",
    "        # summarize history for loss\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(log.history['loss'])\n",
    "        plt.plot(log.history['val_loss'])\n",
    "        plt.title('Model Loss/MSE')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        \n",
    "        # summarize history for rmse\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.plot(log.history['rmse'])\n",
    "        plt.plot(log.history['val_rmse'])\n",
    "        plt.title('Model RMSE')\n",
    "        plt.ylabel('rmse')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "\n",
    "    if (weights != None):\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    predictions = model.predict(X, verbose=1)\n",
    "\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    rmse = sqrt(mse)\n",
    "\n",
    "    print('MSE: %.3f' % mse)\n",
    "    print('RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "47/47 [==============================] - 0s 785us/step\n",
      "MSE: 3235.749\n",
      "RMSE: 56.884\n"
     ]
    }
   ],
   "source": [
    "# Best model error\n",
    "show_info(model, X_train, Y_train, log, weights='weights_rossmann.best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Deep Neural Network is performing better than all the ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model and loaded best weights from file\n"
     ]
    }
   ],
   "source": [
    "# Compile model (required to make predictions)\n",
    "model = create_model()\n",
    "# load weights\n",
    "model.load_weights(\"weights_rossmann.best.hdf5\")\n",
    "print(\"Created model and loaded best weights from file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting using Deep Neural Network\n",
    "Y_test_NN = model.predict(X_test)\n",
    "# Flattening the array\n",
    "Y_test_Final = []\n",
    "for sublist in Y_test_NN:\n",
    "    for item in sublist:\n",
    "        Y_test_Final.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test data\n",
    "test=pd.read_csv(\"CE802_P3_Test.csv\")\n",
    "X_test_Final = test.drop('Target',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>F11</th>\n",
       "      <th>F12</th>\n",
       "      <th>F13</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154.97</td>\n",
       "      <td>9</td>\n",
       "      <td>0.57</td>\n",
       "      <td>USA</td>\n",
       "      <td>-14.34</td>\n",
       "      <td>1286.94</td>\n",
       "      <td>1913.38</td>\n",
       "      <td>-10.54</td>\n",
       "      <td>6.66</td>\n",
       "      <td>232.40</td>\n",
       "      <td>-440.10</td>\n",
       "      <td>12.51</td>\n",
       "      <td>22.99</td>\n",
       "      <td>4</td>\n",
       "      <td>Low</td>\n",
       "      <td>22482.82</td>\n",
       "      <td>1097.098389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78.99</td>\n",
       "      <td>6</td>\n",
       "      <td>2.31</td>\n",
       "      <td>USA</td>\n",
       "      <td>-16.17</td>\n",
       "      <td>1522.99</td>\n",
       "      <td>1458.10</td>\n",
       "      <td>-12.17</td>\n",
       "      <td>4.96</td>\n",
       "      <td>268.26</td>\n",
       "      <td>-328.74</td>\n",
       "      <td>21.03</td>\n",
       "      <td>20.80</td>\n",
       "      <td>12</td>\n",
       "      <td>High</td>\n",
       "      <td>17183.76</td>\n",
       "      <td>3020.926270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>115.81</td>\n",
       "      <td>6</td>\n",
       "      <td>0.24</td>\n",
       "      <td>UK</td>\n",
       "      <td>6.84</td>\n",
       "      <td>979.23</td>\n",
       "      <td>1427.52</td>\n",
       "      <td>-11.22</td>\n",
       "      <td>4.74</td>\n",
       "      <td>233.43</td>\n",
       "      <td>-404.07</td>\n",
       "      <td>1.17</td>\n",
       "      <td>21.42</td>\n",
       "      <td>6</td>\n",
       "      <td>Very high</td>\n",
       "      <td>17585.36</td>\n",
       "      <td>1062.064087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.99</td>\n",
       "      <td>9</td>\n",
       "      <td>1023.63</td>\n",
       "      <td>Rest</td>\n",
       "      <td>-12.75</td>\n",
       "      <td>1052.18</td>\n",
       "      <td>605.80</td>\n",
       "      <td>-9.75</td>\n",
       "      <td>11.46</td>\n",
       "      <td>261.27</td>\n",
       "      <td>-506.25</td>\n",
       "      <td>3.99</td>\n",
       "      <td>19.64</td>\n",
       "      <td>4</td>\n",
       "      <td>High</td>\n",
       "      <td>14621.10</td>\n",
       "      <td>1486.039551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.39</td>\n",
       "      <td>6</td>\n",
       "      <td>1.68</td>\n",
       "      <td>Europe</td>\n",
       "      <td>-10.98</td>\n",
       "      <td>1235.64</td>\n",
       "      <td>-208.92</td>\n",
       "      <td>-11.45</td>\n",
       "      <td>12.76</td>\n",
       "      <td>332.18</td>\n",
       "      <td>-196.89</td>\n",
       "      <td>25.35</td>\n",
       "      <td>19.50</td>\n",
       "      <td>8</td>\n",
       "      <td>Medium</td>\n",
       "      <td>14624.56</td>\n",
       "      <td>3185.894531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>58.13</td>\n",
       "      <td>12</td>\n",
       "      <td>22.05</td>\n",
       "      <td>UK</td>\n",
       "      <td>2.13</td>\n",
       "      <td>1014.70</td>\n",
       "      <td>567.18</td>\n",
       "      <td>-11.91</td>\n",
       "      <td>2.58</td>\n",
       "      <td>266.94</td>\n",
       "      <td>-181.02</td>\n",
       "      <td>12.93</td>\n",
       "      <td>12.35</td>\n",
       "      <td>8</td>\n",
       "      <td>Very high</td>\n",
       "      <td>17222.30</td>\n",
       "      <td>699.596863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>135.11</td>\n",
       "      <td>18</td>\n",
       "      <td>10.14</td>\n",
       "      <td>USA</td>\n",
       "      <td>2.61</td>\n",
       "      <td>390.34</td>\n",
       "      <td>1181.84</td>\n",
       "      <td>-12.15</td>\n",
       "      <td>11.80</td>\n",
       "      <td>224.01</td>\n",
       "      <td>-272.79</td>\n",
       "      <td>9.15</td>\n",
       "      <td>22.21</td>\n",
       "      <td>6</td>\n",
       "      <td>Medium</td>\n",
       "      <td>19004.04</td>\n",
       "      <td>2402.257568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>89.74</td>\n",
       "      <td>6</td>\n",
       "      <td>15.21</td>\n",
       "      <td>Europe</td>\n",
       "      <td>-23.97</td>\n",
       "      <td>1745.60</td>\n",
       "      <td>1240.02</td>\n",
       "      <td>-5.13</td>\n",
       "      <td>1.68</td>\n",
       "      <td>272.01</td>\n",
       "      <td>-331.17</td>\n",
       "      <td>21.99</td>\n",
       "      <td>20.91</td>\n",
       "      <td>10</td>\n",
       "      <td>Very low</td>\n",
       "      <td>15545.40</td>\n",
       "      <td>575.478394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>69.63</td>\n",
       "      <td>24</td>\n",
       "      <td>0.15</td>\n",
       "      <td>USA</td>\n",
       "      <td>-2.28</td>\n",
       "      <td>1764.78</td>\n",
       "      <td>1448.96</td>\n",
       "      <td>-13.66</td>\n",
       "      <td>3.24</td>\n",
       "      <td>197.32</td>\n",
       "      <td>-243.57</td>\n",
       "      <td>2.67</td>\n",
       "      <td>17.72</td>\n",
       "      <td>10</td>\n",
       "      <td>Low</td>\n",
       "      <td>19250.20</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>67.93</td>\n",
       "      <td>12</td>\n",
       "      <td>4.50</td>\n",
       "      <td>Rest</td>\n",
       "      <td>-3.27</td>\n",
       "      <td>1060.63</td>\n",
       "      <td>1378.76</td>\n",
       "      <td>-7.92</td>\n",
       "      <td>9.42</td>\n",
       "      <td>208.50</td>\n",
       "      <td>-355.41</td>\n",
       "      <td>5.46</td>\n",
       "      <td>25.12</td>\n",
       "      <td>4</td>\n",
       "      <td>Very low</td>\n",
       "      <td>14696.28</td>\n",
       "      <td>157.811462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1500 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          F1  F2       F3      F4     F5       F6       F7     F8     F9  \\\n",
       "0     154.97   9     0.57     USA -14.34  1286.94  1913.38 -10.54   6.66   \n",
       "1      78.99   6     2.31     USA -16.17  1522.99  1458.10 -12.17   4.96   \n",
       "2     115.81   6     0.24      UK   6.84   979.23  1427.52 -11.22   4.74   \n",
       "3      48.99   9  1023.63    Rest -12.75  1052.18   605.80  -9.75  11.46   \n",
       "4      71.39   6     1.68  Europe -10.98  1235.64  -208.92 -11.45  12.76   \n",
       "...      ...  ..      ...     ...    ...      ...      ...    ...    ...   \n",
       "1495   58.13  12    22.05      UK   2.13  1014.70   567.18 -11.91   2.58   \n",
       "1496  135.11  18    10.14     USA   2.61   390.34  1181.84 -12.15  11.80   \n",
       "1497   89.74   6    15.21  Europe -23.97  1745.60  1240.02  -5.13   1.68   \n",
       "1498   69.63  24     0.15     USA  -2.28  1764.78  1448.96 -13.66   3.24   \n",
       "1499   67.93  12     4.50    Rest  -3.27  1060.63  1378.76  -7.92   9.42   \n",
       "\n",
       "         F10     F11    F12    F13  F14        F15       F16       Target  \n",
       "0     232.40 -440.10  12.51  22.99    4        Low  22482.82  1097.098389  \n",
       "1     268.26 -328.74  21.03  20.80   12       High  17183.76  3020.926270  \n",
       "2     233.43 -404.07   1.17  21.42    6  Very high  17585.36  1062.064087  \n",
       "3     261.27 -506.25   3.99  19.64    4       High  14621.10  1486.039551  \n",
       "4     332.18 -196.89  25.35  19.50    8     Medium  14624.56  3185.894531  \n",
       "...      ...     ...    ...    ...  ...        ...       ...          ...  \n",
       "1495  266.94 -181.02  12.93  12.35    8  Very high  17222.30   699.596863  \n",
       "1496  224.01 -272.79   9.15  22.21    6     Medium  19004.04  2402.257568  \n",
       "1497  272.01 -331.17  21.99  20.91   10   Very low  15545.40   575.478394  \n",
       "1498  197.32 -243.57   2.67  17.72   10        Low  19250.20     0.000000  \n",
       "1499  208.50 -355.41   5.46  25.12    4   Very low  14696.28   157.811462  \n",
       "\n",
       "[1500 rows x 17 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final ML Output\n",
    "X_test_Final[\"Target\"] = pd.DataFrame({\"Target\": Y_test_Final})\n",
    "X_test_Final.to_csv('CE802_Test_P3.csv', index=False)\n",
    "X_test_Final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
