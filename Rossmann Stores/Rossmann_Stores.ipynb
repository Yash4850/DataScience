{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitted by Group 11:\n",
    "##### Yaswanth - 2003287\n",
    "##### Mohamad - 2004060\n",
    "##### Sreelakshmi - 2004055\n",
    "##### Abhay - 2004349"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "# Data Description:\n",
    "We are provided with historical sales data for 1,115 Rossmann stores. The task is to forecast the \"Sales\" column for the test set. Note that some stores in the dataset were temporarily closed for refurbishment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files\n",
    "train.csv - historical data including Sales,\n",
    "test.csv - historical data excluding Sales,\n",
    "sample_submission.csv - a sample submission file in the correct format,\n",
    "store.csv - supplemental information about the stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement:\n",
    "Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. We need to predict sales based on their unique circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data fields:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Most of the fields are self-explanatory. The following are descriptions for those that aren't.\n",
    "Id - an Id that represents a (Store, Date) duple within the test set\n",
    "Store - a unique Id for each store\n",
    "Sales - the turnover for any given day (this is what you are predicting)\n",
    "Customers - the number of customers on a given day\n",
    "Open - an indicator for whether the store was open: 0 = closed, 1 = open\n",
    "StateHoliday - indicates a state holiday. Normally all stores, with few exceptions, are closed on state holidays. Note that all schools are closed on public holidays and weekends. a = public holiday, b = Easter holiday, c = Christmas, 0 = None\n",
    "SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n",
    "StoreType - differentiates between 4 different store models: a, b, c, d\n",
    "Assortment - describes an assortment level: a = basic, b = extra, c = extended\n",
    "CompetitionDistance - distance in meters to the nearest competitor store\n",
    "CompetitionOpenSince[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
    "Promo - indicates whether a store is running a promo on that day\n",
    "Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
    "Promo2Since[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
    "PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started anew. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c557ee686acf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     raise ImportError(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;31m# Bring in subpackages.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m# pylint: disable=unused-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcluster_resolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_device_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\cross_device_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_lib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcollective_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_device_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import math\n",
    "import seaborn as sns\n",
    "from pandasql import sqldf\n",
    "import matplotlib.pyplot as plt\n",
    "#for removing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reading files\n",
    "train=pd.read_csv(\"train.csv\")\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"test.csv\")\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df=pd.read_csv(\"store.csv\")\n",
    "print(store_df.shape)\n",
    "store_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info() # no null values in train data\n",
    "print(\"----------------------------------------------\")\n",
    "test.info() #few null values in open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Date'] = pd.to_datetime(train['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(15,4))\n",
    "sns.countplot(x='Open',hue='DayOfWeek', data=train,palette=\"husl\", ax=ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many stores are closed on Sundays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date\n",
    "\n",
    "# Create Year and Month columns\n",
    "train['Year']  = train['Date'].apply(lambda x: int(str(x)[:4]))\n",
    "train['Month'] = train['Date'].apply(lambda x: int(str(x)[5:7]))\n",
    "\n",
    "test['Year']  = test['Date'].apply(lambda x: int(str(x)[:4]))\n",
    "test['Month'] = test['Date'].apply(lambda x: int(str(x)[5:7]))\n",
    "\n",
    "# Assign Date column to Date(Year-Month) instead of (Year-Month-Day)\n",
    "train['Date'] = train['Date'].apply(lambda x: (str(x)[:7]))\n",
    "test['Date']  = test['Date'].apply(lambda x: (str(x)[:7]))\n",
    "\n",
    "# group by date and get average sales, and percent change\n",
    "avg_sales   = train.groupby('Date')[\"Sales\"].mean()\n",
    "pct_change_sales = train.groupby('Date')[\"Sales\"].sum().pct_change()\n",
    "\n",
    "fig, (axis1,axis2) = plt.subplots(2,1,sharex=True,figsize=(15,8))\n",
    "\n",
    "# plot average sales over time(year-month)\n",
    "ax1 = avg_sales.plot(legend=True,ax=axis1,marker='o',title=\"Average Sales\")\n",
    "ax1.set_xticks(range(len(avg_sales)))\n",
    "ax1.set_xticklabels(avg_sales.index.tolist(), rotation=90)\n",
    "\n",
    "# plot precent change for sales over time(year-month)\n",
    "ax2 = pct_change_sales.plot(legend=True,ax=axis2,marker='o',rot=90,colormap=\"summer\",title=\"Sales Percent Change\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that sales during december are at peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average sales and customers over years\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n",
    "\n",
    "sns.barplot(x='Year', y='Sales', data=train, ax=axis1)\n",
    "sns.barplot(x='Year', y='Customers', data=train, ax=axis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average sales and customers over days of week\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n",
    "\n",
    "sns.barplot(x='DayOfWeek', y='Sales', data=train, ax=axis1)\n",
    "sns.barplot(x='DayOfWeek', y='Customers', data=train, ax=axis2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales and Customers on Sunday are lowest as many stores are closed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot average sales and customers over months\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n",
    "\n",
    "sns.barplot(x='Month', y='Sales', data=train, ax=axis1)\n",
    "sns.barplot(x='Month', y='Customers', data=train, ax=axis2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sales and Customers are comparatively higher in December as compared to other months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot average sales and customers with/without promo\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n",
    "\n",
    "sns.barplot(x='Promo', y='Sales', data=train, ax=axis1)\n",
    "sns.barplot(x='Promo', y='Customers', data=train, ax=axis2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see without promo the store doesn't stand a chance against stores with promo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# StateHoliday has values 0 & \"0\", So, we need to merge values with 0 to \"0\"\n",
    "train[\"StateHoliday\"]= train[\"StateHoliday\"].replace(0, \"0\")\n",
    "\n",
    "sns.countplot(x='StateHoliday',data=train)\n",
    "\n",
    "# Plot average sales on StateHoliday\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n",
    "\n",
    "sns.barplot(x='StateHoliday', y='Sales', data=train, ax=axis1)\n",
    "filt = (train[\"StateHoliday\"] != \"0\") & (train[\"Sales\"] > 0) # we are taking sales which are more than 0\n",
    "sns.barplot(x='StateHoliday', y='Sales', data=train[filt], ax=axis2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combining a,b and c type stores so as to reduce the bias\n",
    "train[\"StateHoliday\"] = train[\"StateHoliday\"].map({0: 0, \"0\": 0, \"a\": 1, \"b\": 1, \"c\": 1})\n",
    "test[\"StateHoliday\"] = test[\"StateHoliday\"].map({0: 0, \"0\": 0, \"a\": 1, \"b\": 1, \"c\": 1})\n",
    "\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n",
    "plt.title('All Sales of stores')\n",
    "sns.barplot(x='StateHoliday', y='Sales', data=train, ax=axis1)\n",
    "filt = (train[\"Sales\"] > 0) # we are taking sales which are more than 0\n",
    "sns.barplot(x='StateHoliday', y='Sales', data=train[filt], ax=axis2)\n",
    "plt.title('Average Sales of stores which are open')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graphs we can clearly see that the stores which are 'OPEN' during stateholidays have high sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Sales over SchoolHoliday\n",
    "sns.countplot(x='SchoolHoliday',data=train)\n",
    "\n",
    "# Plot average sales on StateHoliday\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\n",
    "\n",
    "sns.barplot(x='SchoolHoliday', y='Sales', data=train, ax=axis1)\n",
    "\n",
    "sns.barplot(x='SchoolHoliday', y='Customers', data=train, ax=axis2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the sales & customers during School Holiday are more compared to normal days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Sales\"].plot(kind='hist',bins=70,xlim=(0,15000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are mostly 0's in this plot because the stores were closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STORE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df.info() # many null values\n",
    "print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_df['PromoInterval'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging train and store_df\n",
    "train_store = train.merge(store_df,left_on=['Store'], right_on=['Store'],how='left')\n",
    "print(train_store.shape)\n",
    "train_store.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_store = test.merge(store_df,left_on=['Store'], right_on=['Store'],how='left')\n",
    "print(test_store.shape)\n",
    "test_store.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_store.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_store.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking correlation bw different variables\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(train_store.corr(),vmax=.7,cbar=True,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highly correlated features with the target variable(Sales) are :- Promo, Open, Customers, DayOfWeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Treating Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_store.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average sales on StateHoliday\n",
    "fig, (axis1,axis2,axis3,axis4,axis5,axis6) = plt.subplots(1,6,figsize=(20,4))\n",
    "plt.title('CompetitionDistance')\n",
    "sns.boxplot( train_store['CompetitionDistance'],ax=axis2)\n",
    "plt.title('CompetitionOpenSinceMonth')\n",
    "sns.boxplot( train_store['CompetitionOpenSinceMonth'], ax=axis3)\n",
    "plt.title('CompetitionOpenSinceYear')\n",
    "sns.boxplot( train_store['CompetitionOpenSinceYear'], ax=axis4)\n",
    "plt.title('Promo2SinceWeek')\n",
    "sns.boxplot( train_store['Promo2SinceWeek'], ax=axis5)\n",
    "plt.title('Promo2SinceYear')\n",
    "sns.boxplot( train_store['Promo2SinceYear'], ax=axis6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see there are lot of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week of year and calculating promo\n",
    "train_store['WeekOfYear'] = pd.DatetimeIndex(train_store['Date']).weekofyear\n",
    "train_store['PromoOpen'] = 12 * (train_store.Year - train_store.Promo2SinceYear) + \\\n",
    "        (train_store.WeekOfYear - train_store.Promo2SinceWeek) / 4.0\n",
    "train_store['PromoOpen'] = train_store.PromoOpen.apply(lambda x: x if x > 0 else 0)\n",
    "train_store.loc[train_store.Promo2SinceYear == 0, 'PromoOpen'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_store['WeekOfYear'] = pd.DatetimeIndex(test_store['Date']).weekofyear\n",
    "test_store['PromoOpen'] = 12 * (test_store.Year - test_store.Promo2SinceYear) + \\\n",
    "        (train_store.WeekOfYear - test_store.Promo2SinceWeek) / 4.0\n",
    "test_store['PromoOpen'] = test_store.PromoOpen.apply(lambda x: x if x > 0 else 0)\n",
    "test_store.loc[test_store.Promo2SinceYear == 0, 'PromoOpen'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_store['CompetitionOpen'] = 12 * (train_store.Year - train_store.CompetitionOpenSinceYear) + (train_store.Month - train_store.CompetitionOpenSinceMonth)\n",
    "test_store['CompetitionOpen'] = 12 * (test_store.Year - train_store.CompetitionOpenSinceYear) + (test_store.Month - test_store.CompetitionOpenSinceMonth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing null values with median\n",
    "med_comp_month = train_store['PromoOpen'].astype('float').median(axis=0)\n",
    "train_store['PromoOpen'].replace(np.nan,math.floor(med_comp_month),inplace=True)\n",
    "\n",
    "med_comp_month = train_store['PromoOpen'].astype('float').median(axis=0)\n",
    "test_store['PromoOpen'].replace(np.nan,math.floor(med_comp_month),inplace=True)\n",
    "\n",
    "med_comp_month = train_store['CompetitionOpen'].astype('float').median(axis=0)\n",
    "train_store['CompetitionOpen'].replace(np.nan,math.floor(med_comp_month),inplace=True)\n",
    "\n",
    "med_comp_month = train_store['CompetitionOpen'].astype('float').median(axis=0)\n",
    "test_store['CompetitionOpen'].replace(np.nan,math.floor(med_comp_month),inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am replacing with Median (instead of mean due to outliers) as they are not highly correlated to the dependent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_store.isnull().sum() # no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Store data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,5))\n",
    "\n",
    "#plt.title('StoreType vs Sales')\n",
    "sns.barplot(x='StoreType', y='Sales', data=train_store, order=['a','b','c', 'd'],ax=axis2)\n",
    "sns.countplot(x='StoreType',data=train_store, order=['a','b','c', 'd'],ax=axis1)\n",
    "#plt.title('StoreType vs Customers')\n",
    "#sns.barplot(x='StoreType', y='Customers', data=train_store, order=['a','b','c', 'd'], ax=axis2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the 'b-type' stores are less but have high sales and volume, while 'a-type' stores are high in number but have relatively low sales and volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.title('Assortment')\n",
    "#sns.countplot(x='Assortment', data=train_store, order=['a','b','c'], ax=axis1)\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,5))\n",
    "#plt.title('Assortment vs Sales')\n",
    "sns.countplot(x='Assortment', data=train_store, order=['a','b','c'], ax=axis1)\n",
    "sns.barplot(x='Assortment', y='Sales', data=train_store, order=['a','b','c'], ax=axis2)\n",
    "#plt.title('Assortment vs Customers')\n",
    "#sns.barplot(x='Assortment', y='Customers', data=train_store, order=['a','b','c'], ax=axis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Promo2')\n",
    "sns.countplot(x='Promo2', data=train_store)\n",
    "\n",
    "fig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,4))\n",
    "\n",
    "plt.title('Promo2 vs Sales')\n",
    "sns.barplot(x='Promo2', y='Sales', data=train_store, ax=axis1)\n",
    "plt.title('Promo2 vs Customers')\n",
    "sns.barplot(x='Promo2', y='Customers', data=train_store, ax=axis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_store['Customers'],color='Black')\n",
    "train_store['Customers'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_store['Sales'],color='Black')\n",
    "train_store['Sales'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlations\n",
    "num_feat=train_store.columns[train_store.dtypes!=object]\n",
    "num_feat=num_feat[1:-1] \n",
    "labels = []\n",
    "values = []\n",
    "for col in num_feat:\n",
    "    labels.append(col)\n",
    "    values.append(np.corrcoef(train_store[col].values, train_store['Sales'].values)[0,1])\n",
    "ind = np.arange(len(labels))\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "rects = ax.barh(ind, np.array(values), color='red')\n",
    "ax.set_yticks(ind+((width)/2.))\n",
    "ax.set_yticklabels(labels, rotation='horizontal')\n",
    "ax.set_xlabel(\"Correlation coefficient\")\n",
    "ax.set_title(\"Correlation Coefficients w.r.t Sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering & Selection of Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_store.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new columns Average Customers and Sales Per Customerfrom pandasql import sqldf\n",
    "avg_customer = sqldf(\n",
    "      \"\"\"\n",
    "      SELECT\n",
    "      Store,\n",
    "      DayOfWeek,\n",
    "      sum(case when Customers is not null then Sales/Customers else 0 end) as SpC,\n",
    "      round(avg(Customers)) Avg_Customers\n",
    "      from train_store\n",
    "      group by Store,DayOfWeek\n",
    "      \"\"\"\n",
    "    )\n",
    "    \n",
    "test_store = sqldf(\n",
    "      \"\"\"\n",
    "      SELECT\n",
    "      t.*,\n",
    "      ac.SpC,\n",
    "      ac.Avg_Customers\n",
    "      from test_store t\n",
    "      left join avg_customer ac on t.Store = ac.Store and t.DayOfWeek = ac.DayOfWeek\n",
    "      \"\"\"\n",
    "    )\n",
    "train_store = sqldf(\n",
    "      \"\"\"\n",
    "      SELECT\n",
    "      t.*,\n",
    "      ac.SpC,\n",
    "      ac.Avg_Customers\n",
    "      from train_store t\n",
    "      left join avg_customer ac on t.Store = ac.Store and t.DayOfWeek = ac.DayOfWeek\n",
    "      \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_store.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_store.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy varibales for DayOfWeek\n",
    "train_dummies  = pd.get_dummies(train_store['DayOfWeek'], prefix='Day')\n",
    "train_dummies.drop(['Day_7'], axis=1, inplace=True)\n",
    "\n",
    "test_dummies = pd.get_dummies(test_store['DayOfWeek'],prefix='Day')\n",
    "test_dummies.drop(['Day_7'], axis=1, inplace=True)\n",
    "\n",
    "train_store = train_store.join(train_dummies)\n",
    "test_store = test_store.join(test_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy varibales for Assortment\n",
    "train_store_dummies  = pd.get_dummies(train_store['Assortment'], prefix='Assortment')\n",
    "train_store_dummies.drop(['Assortment_c'], axis=1, inplace=True)\n",
    "\n",
    "test_store_dummies = pd.get_dummies(test_store['Assortment'],prefix='Assortment')\n",
    "test_store_dummies.drop(['Assortment_c'], axis=1, inplace=True)\n",
    "\n",
    "train_store = train_store.join(train_store_dummies)\n",
    "test_store = test_store.join(test_store_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy varibales for Storetype\n",
    "train_store_dummies  = pd.get_dummies(train_store['StoreType'], prefix='StoreType')\n",
    "train_store_dummies.drop(['StoreType_d'], axis=1, inplace=True)\n",
    "\n",
    "test_store_dummies = pd.get_dummies(test_store['StoreType'],prefix='StoreType')\n",
    "test_store_dummies.drop(['StoreType_d'], axis=1, inplace=True)\n",
    "\n",
    "train_store = train_store.join(train_store_dummies)\n",
    "test_store = test_store.join(test_store_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns from train and test set\n",
    "train_store.drop(['Customers','CompetitionOpenSinceYear','CompetitionOpenSinceMonth','CompetitionDistance','Promo2SinceWeek','Promo2SinceYear','PromoInterval','WeekOfYear','Year','StoreType','Assortment','Date'],axis=1,inplace=True)\n",
    "train_store['Open'] = train_store['Open'].astype(float)\n",
    "test_store.drop(['Year','CompetitionOpenSinceYear','CompetitionOpenSinceMonth','CompetitionDistance','Promo2SinceWeek','Promo2SinceYear','PromoInterval','Promo2SinceWeek','Promo2SinceYear','PromoInterval','WeekOfYear','WeekOfYear','StoreType','Assortment','Date'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_store.info())\n",
    "print(test_store.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_store[test_store['Open'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see all the null values of open are when week is not 7 and StateHoliday and SchoolHoliday are 0's. So replacing them with 1's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN values in test with Open=1\n",
    "test_store[\"Open\"].fillna(1, inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping DayOfWeek\n",
    "train_store.drop(['DayOfWeek'], axis=1,inplace=True)\n",
    "test_store.drop(['DayOfWeek'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing all rows(stores) that were closed as the sales are 0 when store is closed and it tend to make the model unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows(store,date) that were closed\n",
    "train_store= train_store[train_store[\"Open\"] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving id's of those stores which were closed so we can put 0 in their respective sales column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving id's of those stores which were closed so we can put 0 in their respective sales column\n",
    "closed_ids = test_store[\"Id\"][test[\"Open\"] == 0].values\n",
    "print(closed_ids.shape)\n",
    "closed_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all rows(store,date) that were closed\n",
    "test_store = test_store[test_store[\"Open\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_store = test_store.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_store.drop([\"index\"],axis =1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and testing sets\n",
    "train = train_store.drop([\"Store\",\"Open\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_store.drop([\"Id\",\"Store\",\"Open\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train_data(scaler_x, scaler_y):\n",
    "    '''\n",
    "    Transform train data set and separate a test dataset to validate the model in the end of training and normalize data\n",
    "    '''\n",
    "    X_train = train.drop([\"Sales\"], axis=1) # Features\n",
    "    y_train = np.array(train[\"Sales\"]).reshape((len(X_train), 1)) # Targets\n",
    "    X_train = scaler_x.fit_transform(X_train)\n",
    "    y_train = scaler_y.fit_transform(y_train)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.20, random_state=42)\n",
    "\n",
    "    return (X_train, y_train), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    '''\n",
    "    Remove column of predictions and normalize data of submission test data set.\n",
    "    '''\n",
    "    X_test = test # Features\n",
    "    X_test = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOW! That took a lot of time, but we finally have clean and perfect data that we want. Now to the model building part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe_val(y_true, y_pred):\n",
    "    '''\n",
    "    RMSPE calculus to validate evaluation metric about the model\n",
    "    '''\n",
    "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true), axis=0))[0]\n",
    "def rmse(y_true, y_pred):\n",
    "    '''\n",
    "    RMSE calculus to use during training phase\n",
    "    '''\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmspe(y_true, y_pred):\n",
    "    '''\n",
    "    RMSPE calculus to use during training phase\n",
    "    '''\n",
    "    return K.sqrt(K.mean(K.square((y_true - y_pred) / y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We took Relu activation function as it is from 0 to infinity\n",
    "def create_model():\n",
    "    '''\n",
    "    Create a neural network\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, input_dim=X_train.shape[1], activation=\"relu\", kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.2)) # We are dropping a few neurons for generalizing the model\n",
    "    model.add(Dense(32, input_dim=X_train.shape[1], activation=\"relu\", kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, input_dim=X_train.shape[1], activation=\"relu\", kernel_initializer='normal'))\n",
    "    model.add(Dropout(0.2)) \n",
    "    model.add(Dense(1, activation=\"linear\", kernel_initializer='normal'))\n",
    "    adam = Adam(lr=1e-3, decay=1e-3)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=adam, metrics=[rmse, rmspe])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and load data to train the model\n",
    "batch_size = 32\n",
    "nb_epoch = 100\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "print('Loading data...')\n",
    "(X_train, y_train), (X_test, y_test) = load_train_data(scaler_x, scaler_y)\n",
    "\n",
    "print('Build model...')\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Fit model...')\n",
    "filepath=\"weights_rossmann.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min')\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "log = model.fit(X_train, y_train,\n",
    "          validation_split=0.20, batch_size=batch_size, epochs=nb_epoch, shuffle=True, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def show_info(model, X, y, log, weights = None):\n",
    "    '''\n",
    "    Show metrics about the evaluation model and plots about loss, rmse and rmspe\n",
    "    '''\n",
    "    if (log != None):\n",
    "        # summarize history for loss\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['loss'])\n",
    "        plt.plot(log.history['val_loss'])\n",
    "        plt.title('Model Loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        \n",
    "        # summarize history for rmse\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['rmse'])\n",
    "        plt.plot(log.history['val_rmse'])\n",
    "        plt.title('Model RMSE')\n",
    "        plt.ylabel('rmse')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        print('\\n')\n",
    "        \n",
    "        # summarize history for rmspe\n",
    "        plt.figure(figsize=(14,10))\n",
    "        plt.plot(log.history['rmspe'])\n",
    "        plt.plot(log.history['val_rmspe'])\n",
    "        plt.title('Model RMSPE')\n",
    "        plt.ylabel('rmspe')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "    if (weights != None):\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    predictions = model.predict(X, verbose=1)\n",
    "\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    rmse = sqrt(mse)\n",
    "    rmspe = rmspe_val(y, predictions)\n",
    "\n",
    "    print('MSE: %.3f' % mse)\n",
    "    print('RMSE: %.3f' % rmse)\n",
    "    print('RMSPE: %.3f' % rmspe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_info(model, X_test, y_test, log, weights='weights_rossmann.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = load_test_data()\n",
    "predict = model.predict(test_data)\n",
    "predict = scaler_y.inverse_transform(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=pd.DataFrame(predict,columns = ['Sales'])\n",
    "submission = pd.concat([test_store['Id'],pred],axis=1)\n",
    "# Creating closed stores dataframe\n",
    "Closed_Stores = pd.DataFrame(closed_ids,columns = ['Id'])\n",
    "print(Closed_Stores.shape)\n",
    "Closed_Stores['Sales'] = 0\n",
    "submission = submission.append(Closed_Stores)\n",
    "# Converting it to csv\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We submitted and got an error of 0.18"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
